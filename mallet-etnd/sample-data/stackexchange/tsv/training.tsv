1	physics	 I often hear about subatomic particles having a property called "spin" but also that it doesn't actually relate to spinning about an axis like you would think. Which particles have spin? What does spin mean if not an actual spinning motion? 
2	physics	 How would you explain string theory to non physicists such as myself? I'm specially interested on how plausible is it and what is needed to successfully prove it? 
3	physics	 This is a question that has been posted at many different forums, I thought maybe someone here would have a better or more conceptual answer than I have seen before: Why do physicists care about representations of Lie groups? For myself, when I think about a representation that means there is some sort of group acting on a vector space, what is the vector space that this Lie group is acting on? Or is it that certain things have to be invariant under a group action? maybe this is a dumb question, but i thought it might be a good start... To clarify, I am specifically thinking of the symmetry groups that people think about in relation to the standard model. I do not care why it might be a certain group, but more how we see the group acting, what is it acting on? etc. 
4	physics	 Imagine going to the rest frame of a massive particle. In this frame, there is rotational symmetry, which means that the Lie algebra of rotations acts on the wave function. So the wave function is a vector in a representation of Lie(SO(3)) = Lie(SU(2)). "Spin" is the label of precisely which representation this is. Note that while SO(3) and SU(2) share a Lie algebra, they are different as groups, and it is a fact of life ("the connection between spin and statistics") that some particles -- fermions, with half-integral spin -- transform under representations of SU(2) while others -- bosons, with integral spin -- transform under SO(3). 
5	physics	 Spin is a technical term specifically referring to intrinsic angular momentum of particles. It means a very specific thing in quantum/particle physics. (Physicists often borrow loosely related everyday words and give them a very precise physical/mathematical definition.) Since truly fundamental particles (e.g. electrons) are point entities, i.e. have no true size in space, it does not make sense to consider them 'spinning' in the common sense, yet they still possess their own angular momenta. Note however, that like many quantum states (fundamental variables of systems in quantum mechanics,) spin is quantised ; i.e. it can only take one of a set of discrete values. Specifically, the allowed values of the spin quantum number are non-negative multiples of 1/2. The actual spin momentum (denoted ) is a multiple of Planck's constant, and is given by $S = \sqrt{s (s + 1)}$. When it comes to composite particles (e.g. nuclei, atoms), spin is actually fairly easy to deal with. Like normal (orbital) angular momentum, it adds up linearly. Hence a proton, made of three constituent quarks, has overall spin 1/2. If you're curious as to how this (initially rather strange) concept of spin was discovered, I suggest reading about the Stern-Gerlach experiment of the 1920s. It was later put into the theoretical framework of quantum mechanics by Schrodinger and Pauli. 
7	physics	 What are the main problems that we need to solve to prove Laplace's determinism correct and overcome the Uncertainty principle? 
8	physics	 The vector space that is being acted on typically is a Hilbert space of states in quantum mechanics; very roughly, there's a basis of this vector space which is in one-to-one correspondence with the set of possibilities for a physical system. The simplest example to try to get your head around is that of the spin 1/2 particle (2 dim representation of SU(2)), which is explained in any introductory quantum mechanics book. 
9	physics	 Hamilton's principle states that a dynamic system always follows a path such that its action integral is stationary (that is, maximum or minimum). Why should the action integral be stationary? On what basis did Hamilton state this principle? 
10	physics	 Some of the leaders in the field are Brian Greene and Michio Kaku. Both have made some Sci channel or Nova series that seemed appropriate for non-physicists. I haven't been following the latest in physics for a while, but I thought that M-Theory had supplanted string theory some time ago. If so, then Kaku would be the guy. Greene's series and book "The Elegant Universe" also goes into some Quantum mechanics, which might help as well. These are really the only two players I know of that simplify string theory. Does anyone else have some ideas? 
13	physics	 I've been using the term "sound" all my life, but I really have no clue as to what sound exactly is or how it is created. What is sound? How is it produced? Can it be measured? 
14	physics	 As you can see from the image below, you want the variation of the action integral to be a minimum, therefore $\displaystyle \frac{\delta S}{\delta q}$ must be $0$. Otherwise, you are not taking the true path between $q_{t_{1}}$ and $q_{t_{2}}$ but a slightly longer path. However, even following $\delta S=0$, as you know, you might end up with another extremum. Following the link from j.c., you can find On a General Method on Dynamics , which probably answers your question regarding Hamilton's reasoning. I haven't read it but almost surely it is worthwhile. 
15	physics	 I know that there's big controversy between two groups of physicists: those who support string theory (most of them, I think) and those who oppose it. One of the arguments of the second group is that there's no way to disprove the correctness of the string theory. So my question is if there's any defined experiment that would disprove string theory? 
16	physics	 What it sound? Sound is nothing but a mechanical wave. The human ear can detect mechanical waves from 20 Hz to 20,000 Hz. How is it produced? It is produced when energy transformation occurs. For eg. beating a drum - mechanical energy. Can it be measured? Yes, it can be measured by the frequency of the wave. 
17	physics	 Why the sky is blue during the day, red during sunrise/set and black during the night? 
18	physics	 Experiments such as the double-slit, EPR and Bell inequality experiments show that quantum mechanics is not just a computational tool, but a genuine property of nature. In this sense, Heisenberg's Uncertainty Principle cannot be overcome and there will be no reversal to determinism. 
19	physics	 Physicists often refer to the energy of collisions between different particles. My question is: how is that energy calculated? Is that kinetic energy? Also, related to this question, I know that the aim is to have higher and higher energy collisions (e.g to test for Higgs Boson). My understanding is that to have higher energy you can either accelerate them more, or use particles with higher mass. Is this correct? 
20	physics	 In general usage, "sound" refers to our perception of the vibrations of particles (atoms, molecules) in some medium, typically air or water, though sound waves can travel through any medium. Vibrations are produced whenever objects cause the particles in the medium to oscillate, e.g. clapping your hands together, beating a drum, or yelling. Sound can be measured the way it is heard - just as air molecules begin to vibrate in the presence of a vibrating object - the air molecules can cause other things to vibrate (such as our eardrums, or a part of a microphone) and we can quantify the level of sound by measuring the motion of the detector in any number of ways. At a deeper level, sound waves are the "Goldstone modes" corresponding to broken translational symmetry of the surrounding medium. Let me try to bring this a little more down to earth. Think about a parcel of air, large enough to contain a lot of air molecules, but not so large that there are sizable air currents swirling within. We can think of this in an ideal situation as being a fluid of uniform density. This fluid has translational symmetry - if I translate it a little in any direction, it looks more or less the same "in the bulk". Translational symmetry is a continuous symmetry, so I can squeeze the air in a spatially-varying way. It turns out that if I do this in a wavelike pattern with a long wavelength, this is an excitation that costs very little energy, and hence will be important to the physics. These waves are precisely sound waves. This probably doesn't make much sense as I've written it, so I may try to edit this a bit later on if anyone is actually interested in this point of view. 
21	physics	 Where is the Monte Carlo method used in physics? 
22	physics	 The keywords here are Rayleigh scattering . See also diffuse sky radiation . But much more simply, it has to do with the way that sunlight interacts with air molecules. Blue light is scattered more than red light, so during the day when we look at parts of the sky that are away from the sun, we see more blue than red. During sunset or sunrise, most of the light from the sun comes towards the earth at a sharp angle, so now the blue light is mostly scattered away, and we see mostly red light. 
23	physics	 Sound is air molecules hitting each other, bouncing back, then again hitting each other, and so on. This back-and-forth movement is called vibration. Sound is produced when something like a drum vibrates, and starts the aforementioned bouncing of air molecules. Sound is picked up inside your ears by eardrums, which are like reverse-drums, in that they vibrate with the air molecules. This vibration is then converted to biochemical signals which travel to your brain. There is a Mickey Mouse visualization here on Youtube: 
24	physics	 I think it's clear enough that if you turn your bicycle's steering wheel left, while moving, and you don't lean left, the bike will fall over (to the right) as you turn. I figure this is because the bike's momentum keeps it moving in the direction you were going, and since your wheels have friction against the ground, the top of the bike moves forward relative to the bottom of the wheels. The top of the bike going north while the bottom of the wheels go northwest will understandable cause you to topple. So to counteract this and keep you from falling over, leaning into the turn is necessary. But is there also a causal relationship -- that leaning will cause the bike to start to turn? If I start leaning left, I will turn left... but maybe that's because I know that if I don't turn the steering wheel left, the bike will fall over (to the left). I experimented with unruly turns of the steering wheel when I was a kid, and got my scrapes and bruises. Now that I'm a cautious and sedate adult I'm not anxious to experiment that way. :-) (I also want to ask why airplanes bank into a turn... they don't have the same issues as a bike, i.e. the bottom part has no special friction against the ground. But that would probably make the question too broad.) 
25	physics	 In statistical mechanics , the Monte Carlo method is used to sample the state space of a statistical system in order to compute physical quantities of interest by averaging. 
26	physics	 I am wondering if someone could provide me with a formula that would tell me at what velocity a projectile can be launched from something using an electromagnetic field. The idea is much like a rail gun or Gauss rifle, but not exactly. Just looking to find a formula to determine the speed of the projectile. Thank you, Michael Vanderpool 
27	physics	 We've learned that the wave function of a particle collapses when we measure a particle's location. If it is found, it becomes more probable to find it a again in the same area, and if not the probability to finding it in the place that was checked decreases dramatically. My question is about the definition of measurement. What makes a measurement different from any other interaction between two particles (gravity and EM fields for example)? In reality, almost every particle interacts with any other particle, so shouldn't there be constant collapse of the wave function all the time? If this happens we're right back in classical mechanics, aren't we? 
28	physics	 It is worth remembering that in at velocities close to c. A particle kinetic energy is inter-wind with its rest mass. So the actual energy equation is $E = \sqrt{p^2c^2+m^2c^4}$ So the energy of a collision is the sum of the above energy for the two colliding particles. That is why you construct improved accelerators. As it is the only way to tune up the energy. To the extend of my knowledge we don't how to tune up mass. 
29	physics	 I recently encountered a puzzle where a person drove 120 miles at 40mph, then drove back the same 120 miles at 60mph. The average of the speeds is (40mph+60mph)/2 = 50mph, so the total trip time should be 240mi/50mph = 4.8 hours. But the trip actually took 5 hours. Why is that, and what is the correct way to calculate average speed? 
30	physics	 What you describe in your question is the "Copenhagen interpretation" of quantum mechanics. There are more nuanced views of this nowadays that don't treat "measurements" quite so asymmetrically, see e.g. sources that talk about decoherence. I recommend watching the classic lecture "Quantum Mechanics in your face" by Sidney Coleman for a nice take on this sort of thing. 
31	physics	 What is Einstein's theory of Special relativity , in terms a lay person can follow? 
32	physics	 There is a common myth that water flowing out from a sink should rotate in direction governed by on which hemisphere we are; this is shown false in many household experiments, but how to show it theoretically? 
33	physics	 The reason is because the time taken for the two trips are different, so the average speed is not simply $\frac{v_1 + v_2}{2}$ We should go back to the definition. The average speed is always (total length) &divide; (total time). In your case, the total time can be calculated as \begin{align} \text{time}_1 &amp;= \frac{120 \mathrm{miles}}{40 \mathrm{mph}} \\\\ \text{time}_2 &amp;= \frac{120 \mathrm{miles}}{60 \mathrm{mph}} \end{align} so the total time is $120\mathrm{miles} \times \left(\frac1{40\mathrm{mph}} + \frac1{60\mathrm{mph}}\right)$. The average speed is therefore: \begin{align} \text{average speed} &amp;= \frac{2 \times 120\mathrm{miles}}{120\mathrm{miles} \times \left(\frac1{40\mathrm{mph}} + \frac1{60\mathrm{mph}}\right)} \\\\ &amp;= \frac{2 }{ \left(\frac1{40\mathrm{mph}} + \frac1{60\mathrm{mph}}\right)} \\\\ &amp;= 48 \mathrm{mph} \end{align} In general, when the length of the trips are the same, the average speed will be the harmonic mean of the respective speeds. $$ \text{average speed} = \frac2{\frac1{v_1} + \frac1{v_2}} $$ 
34	physics	 The difficulty is that since the trip at 40mph takes longer, you spend more time going 40mph than you do going 60mph, so the average speed is weighted more heavily towards 40 mph. When calculating average speeds for fixed distances, it is better think of everything in minutes per mile rather than miles per hour. 60 miles per hour is 1 minute per mile, while 40 miles per hour is 1.5 minutes per mile. Since we travel the same number of miles at each speed, we can now take the mean of these two figures. That's 1.25 minutes per mile on average. For 240 miles total, 240miles*1.25minutes/mile = 300 minutes = 5 hours. This method is called finding the "harmonic mean" of the speeds. 
35	physics	 If I separate two magnets whose opposite poles are facing, I am adding energy. If I let go of the magnets, then presumably the energy that I added is used to move the magnets together again. However, if I start with two separated magnets (with like poles facing), as I move them together, they repel each other. They must be using energy to counteract the force that I'm applying. Where does this energy come from? 
36	physics	 $$\mathrm{Average\ Speed = \frac{Total\ Distance}{Total\ time}}$$ So basically, $t_1 = 120/40 = 3\ hrs$ $t_2 = 120/60 = 2\ hrs$ Total time $= 5\ hrs$ Total distance = $240$ miles Average speed$ = 240/5 = 48\ mph$ 
37	physics	 Physicists studying the grounds of physics and some mathematicians often come to a theories which are like a general-relativity, but do not coincide with it. Often these theories contradict the real world. What should one do in order to check one of such theories? Which papers/books should one read in order to be able to understand about some mathematical theories, that "the mathematical theory is nice, but this is not our world"? 
38	physics	 For example, a Markov Chain Monte Carlo (MCMC) method was used in the WMAP [1] papers, the most important measurment/papers of modern CMB [2] cosmology, one of the pillars of the Big Bang Theory of the Universe. MCMC was used to explore and fit the parameters of the theory with the WMAP (and other) measurements. You can download the technical papers at: [1] Wilkinson Microwave Anisotropy Probe [2] Cosmic Microwave Background 
39	physics	 As is said in a comment, the reasoning in the first paragraph is correct but the one in the second paragraph is wrong. If you apply a force on something without "moving" the work is null and there is no energy exchange involved (this is not the same thing than doing that with your muscles, but that's another story :p). ( Work = integral[a to b] of F dot dx ; so Work = 0 if there is not "circulation"). Thus the magnets do not need any energy to statically counteract the force. However, if you do move the magnets, then you need to give some energy. This energy is stored in the system because you cause a variation of magnetic flux: magnet 1 moving induce a variation of flux seen by magnet 2, and this will change the state of magnet 2, increasing its potential energy. 
40	physics	 Magnetic field in this case (a set of magnets in space, no relativity involved) is conservative, which means it has a potential -- each positional configuration of charges (or dipoles in this case) has its fixed energy which does not depend on history or momenta of charges. So, the work you put or get from displacing them is just exchanged with the potential energy of the field, which means no energy is created or destroyed, just stored. 
41	physics	 There have been recent results in mathematics regarding Conformal field theories and topological field theories. I am curious about the reaction to such results in the physics community. So I guess a starting place for a question would be something like: Did Atiyah's axiomatization of TQFT capture what physicists feel are important in QFT's? How many more parameters are there to what a physicist would call a QFT or FT in general? Perhaps I should mention that mathematicians (the ones who dont know any phsyics) think about various Field Theories as things that assign invariants to manifolds that should be sensitive to certain data (depending on the word proceeding field). A lot has been done when you only care about the basic topology of the Manifold you are evalutating your field theory at. Have mathematicians gotten rid of too much for physicists to care? What types of things do physicists want a Field theory to keep track of, what kind of structure on the manifold that is? 
42	physics	 The notes from week 1 of John Baez's course in Lagrangian mechanics ( ) give some insight into the motivations for action principles. The idea is that least action might be considered an extension of the principle of virtual work. When an object is in equilibrium, it takes zero work to make an arbitrary small displacement on it. I.E. the dot product of any small displacement vector and the force is zero (in this case because the force itself is zero). When an object is accelerating, if we add in an "inertial force" equal to -m*a, then a small, arbitrary, time-dependent displacement from the objects true trajectory would again have zero dot product with F-ma, the true force and inertial force added. This gives $(F-ma)\cdot \delta q(t) = 0$ From there, a few calculations found in the notes lead to the stationary action integral. Baez discusses D'Alembert more than Hamilton, but either way it's an interesting look into the origins of the idea. 
43	physics	 The calculation of the Coriolis force is dependent on latitude: $F = m a$ where $a = 2 \Omega sin(lat)$, with $\Omega$ being the Earth's angular velocity $m$ is the mass of the object in question The Earth's angular velocity is (about) $7.29 \times 10^{-5}$ rad/sec So, for a sink with a couple gallons of water in it at 45 degrees north... the Coriolis force is about $7.57 \times 2 \times 7.29 \times 10^{-5} = 1.10 \times 10^{-3}$ N. Interesting reading about this over this way , and (of course) here . 
44	physics	 Special Relativity derives from two basic ideas: The speed of light (in a vacuum) is always c. The laws of physics are the same in all inertial reference frames (basically, points of view that aren't accelerating, that is, they obey Newton's Laws.) With these two points and a little math, various proven conclusions may be derived: Time Dilation : When something moves fast relative to something else, time for the faster moving body slows down. It's not an illusion of time slowing down, it's the real thing: individual atoms that make up the body operate slower, chemical reactions function slower, and biological processes (aging) occur slower. From the perspective of the faster moving body, its time progresses at the usual pace. Length Contraction : Objects moving fast relative to other objects shrink along the line of the direction they're moving. Relativistic Simultaneity : There's no such thing as simultaneous events: because time is attached to the observer, different people could witness 2 events happening in different order. The exception to this is "causally-related" events which are events where event A is the cause of event B. Mass-Energy : The math goes into describing the mass of bodies at rest and how that mass changes as the bodies move. As bodies speed up they get "heavier." Nothing with mass can travel faster than light (and nothing with mass can travel AT the speed of light) because any massive body would reach infinite "relative mass" at that speed. You can derive $E=mc^2$ and fission/fusion from this. This is a very quick summary of the basic points and principles. 
45	physics	 I think your question is divided in two parts. When talking about energy, in the field of HEP or accelerator physics we can talk about total energy kinetic energy momentum As for elementary particles relativistic effects manifest themselves almost all the time, you need to use a relativistic form for the energy: $$E = \sqrt{p^2c^2+m^2c^4}$$ Instead of a bare $E = p^2/2m$ (as in classical mechanics, where $p = m v$). This relation as two parts: one depending on the momentum and one (constant) given by the mast of the particle. It should be also noted that for ultra-relativistic cases, where $E \gg E_0$, we have $E = p c$. Usually for low energy applications, like linear accelerators or low energy experiments we talk about the kinetic energy, which is $E_k = \sqrt{p^2c^2+m^2c^4} - mc^2$. For example if you talk about protons of 160 MeV obviously it is kinetic energy, as the rest mass of the proton is rhougly 1 GeV. For higher energy application usually you can make the ultra-relativistic approximation and you then talk about total energy (in eV) or about the momentum in $\mathrm{eV}/c$; taking $c=1$, both are numerically equal. When you are not sure about which approximation you can take, it is better to explain which one you take. Example: For a proton in the LHC with a momentum of 3.5 TeV/c you can calculate its total energy which is ... How is this energy "calculated"? (I assume you meant "experimentally" or something like that.) In HEP physics we use what we call electron-volt as a unit of energy. A particle of unit charge will have an energy of 1 eV if it descends from rest a potential difference of 1 V. So for example, when you accelerate protons in the LHC, if you have cavities giving you 10 MV, the particle will gain 10 MeV every turns. 
46	physics	 Laplace's determinism is not physically correct over long periods of time. That is, it neglects chaos/"sensitive dependence on initial conditions"/exponential growth of microscopic perturbations already in Newtonian dynamics, which was seriously thought about only in the 20th century. Being true, this also will not be overcome. Stochasticity enters some classical dynamical paths with time. There is subtlety here. In classical mechanics, or the evolution of the wave function, there is a kind of microdeterminism, so that what occurs in the next instant is fully determined by what occurred up until that point. It is in the longer time evolution of a chaotic system that stochasticity creeps in. By the way, Lapalace said "We ought to regard the present state of the universe as the effect of its antecedent state and as the cause of the state that is to follow." This part remains true in chaotic classical mechanics. However, he then continued "An intelligence knowing all the forces acting in nature at a given instant, as well as the momentary positions of all things in the universe, would be able to comprehend in one single formula the motions of the largest bodies as well as the lightest atoms in the world, provided that its intellect were sufficiently powerful to subject all data to analysis; to it nothing would be uncertain, the future as well as the past would be present to its eyes. The perfection that the human mind has been able to give to astronomy affords but a feeble outline of such an intelligence. (Laplace 1820)" This is the part that classical chaos invalidates. You might also read Finally, there are also questions whether, in light of general relativity, black holes, etc, we can even speak of a "state of the universe as a whole" There may not be such a god's eye view altogether. These issues need a philosophy forum, however. 
47	physics	 Einstein's Special Theory of Relativity cannot be of much significance to a lay person because he has no use for such knowledge. However, the common idea that this topic is extremely complicated motivates the layman to learn it. The basic concepts of this theory are actually somewhat simple. In short, when an object is moving a certain velocity $u$, a few seemingly unusual phenomena happen to it. If a bar of length $L_0$ moves in the direction of its length, its new length will appear to be $L_0 \sqrt{1 - u^2 / c^2}$ from the our viewpoint on the ground, where $c$ is the speed of light. If that formula means nothing to you, simply take note that the length decreases as the velocity increases. Of course, from the bar's viewpoint, it will appear that our length has changed by that factor. Since time is proportional to distance traveled at a constant velocity, the length contraction can show that the "rate of time" also changes when an object is moving. If a clock is traveling through space close to the speed of light, it will tick significantly slower than it did at rest. Those are the basics, but phenomena such as mass-energy equivalence and nuclear binding energy can be derived from these concepts and experiments conducted in the early 20th century. 
48	physics	 String theory should come with a proposal for an experiment, and make some predictions about the results of the experiment; then we could check against the real results. If a theory cannot come with any predictions, then it will disprove itself little by little ... The problem is that, with string theory, this is extremely difficult to do, and string theorists have year in front of them to go in that direction; but if in 100 years we are still at the same status, then it would be a proof that string theory is unfruitful... 
49	physics	 What should be a properties of a body so a contact with it can be detected by a capacitive touchscreen or a touchpad? 
50	physics	 As others have pointed out, the question is somewhat misworded. The correct question is probably just "Why do magnets repel?", which can be traced back to the question of why electromagnetism (EM) is a fundamental force of nature. Like gravity, it's one of the four known fundamental forces of nature. Regarding the use of the word "energy". Energy is the same thing as work. Work = Force x distance. If there is a 10N [Newton] force acting on a ball, and you move the ball by 1m [meter], you have done 10N x 1M = 10Nn = 10J [Joule] of work (provided the force is constant). So, if you move the magnetic poles against each other, you do work (against the EM field). Then, if you attach the magnets to a spring and let go, the springs will compress. The 10J of work you invested is now stored in the springs! Then, if you remove the magnets and put a bullet on the spring, and let go, you can use the 10J of energy to launch the bullet (work is converted to kinetic energy), and so on =) What happens when you push two magnets against each other, but then just hold them in place? Per the above definition of Work = Force x distance, you're not doing work, because distance = 0. But clearly your muscles are straining, and you're burning calories (which is just another unit of energy or work, like Joules), so what's going on? From a physics textbook point of view, no work is being done. This is similar to placing an apple on a table. The table is countering the force of gravity, like your muscles in the magnet example. The table has no muscles, because it is made of wood, and it won't collapse under the weight of the apple. In reality, the stiffness of the table/wood is provided by the electromagentic interactions of the atoms making up the table, so in the end it is the EM force counteracting the gravitational force. In the original magnet/muscle example, your arms are not made up of wood, so your body is doing biochemical/mechanical work to stiffen the cells in your arms by burning the food you ate. (The fact that this process is not perfect means that heat is a by product of this process, that's why your muscles get hot and you start sweating.) 
51	physics	 Physical theories are rarely constructed as a purely theoretical exercise - they usually are based on empirical data which is fitted a mathematical solution. The physical theory then tries to explain the data and corresponding math. The basis of scientific theory is that it must be verified by repeatable tests and data. Sometimes theories that explain some data well are disproven when applied to other situations - and sometimes amendments are made in order to account for the difference. That is the nature of every scientific theory - it is accepted until real world data says it is wrong. There is no way to know whether any theory truly describes our world or not - it holds until it no longer matches observations. 
52	physics	 A few years ago I went to a museum, where there was a board with 2 bar magnets, on a pole each so they could rotate. If you rotated them so the lined up with the same poles (N) facing each other, they'd repel spinning until the south poles lined up, then they'd repel and spin so the north poles lined up and so. This was without any extra help, once you lined them up they would just keep spinning by their selves. How exactly is this possible that the 2 magnets kept producing kinetic energy? 
53	physics	 Greene's book has already been mentioned, but Nova's The Elegant Universe is a somewhat informative 3-hour video series. Obviously, to truly understand the theory's progress, years of mathematics and physics must be studied, preferably at a university. 
54	physics	 Determinism as more to worry about stochastic and chaotic processes than with the Heisenberg's uncertainty principle. As a simple proof, you can easily consider a purely classical system and observe that you don't have determinism as Laplace had hopped. 
55	physics	 They are not producing energy; their movement is a result of the work you've put during setting same poles to face each other. They will eventually stop, since this energy is dissipated by friction on their fixings and at some point their momentum will decrease to the point it will be not enough to push the system through energy pick when the same poles are nearby. After this, they will relax to their energy optimum, so probably into configuration when different poles are facing each other. 
56	physics	 String theory, was constructed with the idea that at low energies it should reduce to the quantum mechanical and particle world that we see every day. This is analogous to the correspondence principle in quantum mechanics. In some sense, any experiment that discredits quantum mechanics will cause a serious re-examination of string theory, however, as many experimentalists will joke, a theorist will always find a way to fix his theory to match the observations. In any case, quantum theory seems very well supported and is unlikely that it will be discredited any time soon Direct tests of string theory, however, will have to wait until we can probe much higher energies. 
57	physics	 Another important use of Monte-Carlo is in nuclear engineering: basically for two distinct things: nuclear reactor core calculations and radiation shielding. The first use Monte-Carlo to obtain precise estimations of the reactivity of the core (and other parameters) and the second allow to calculate, for example, the thickness of a shielding. More generally this is related to particle-matter interaction : this is a process where each interaction is quantified by its cross-section which then gives a probability for the interaction to occur. This is thus a very good field to apply MC methods. Another example is in medical physics, for the calculation of the deposited dose on a patient. 
58	physics	 As KennyTM stated, use the conservation of energy. Say for example you have a constant electric field E to accelerate your particle of charge q and mass m, this will mean that the electrical energy E d q accumulated over the distance d will convert entirely into kinetic energy $\frac12 m v^2$ (assuming you stay non-relativistic, otherwise it's $\sqrt{m^2c^4+p^2c^2}$). Just solve the equation for v (or the momentum p in the relativistic case). Things get a bit more complicated for a non-static EM field. 
59	physics	 First, from your comment: a magnetic field will never accelerate anything (I am more specifically thinking about a charge "particle" as this is trivial for a golf ball) ( Edit : see the comments bellow about that statement). The magnetic field is always perpendicular to the trajectory, thus giving no contribution in a change of the kinetic energy. To accelerate a particle, you need an electric field. Then the way to proceed is the following: if you know the electric field and the path, you can integrate E dot dx and multiply by the charge. This will give you an energy difference, and this energy is added as kinetic energy, thus from the kinetic energy, if you are non relativistic you can use $E_k = \frac{1}{2} m v^2$ and make a difference. 
61	physics	 If you just lean without holding the steering wheel, that will automatically turn in order to conserve the total angular momentum. If you hold it tight however, this is not possible and you will crash. 
62	physics	 The LHC in Geneva is a circular accelerator, 27 km long, why is it like that ? 
63	physics	 Laplace's determinism is fine if you do no longer consider x(t) as the function you want to deterministically describe but accept the fact that only the wave function $\psi(x,t)$ is deterministic - and even that only when you neglect QFT effects. 
64	physics	 To accelerate particles to very high energies, you need a long path, because the force is limited (just as in case of an airplane runway). You run them in circles to have it confined, or you would have to build something the size of a solar system. There are, however, what is called Linear Accelerators, like SLAC. These are used to accelerate electrons, which are too light to be run in circles. They radiate a lot of energy when pushed sideways. 
65	physics	 Cyclotrons, while generally economic because of the reduced space and cost to build (compared to linear accelerators), suffer from two notable problems. Note that the LHC is in fact a synchotron , which is notably improved over a cyclotron for relativistic (high energy) particles. The following principles still apply pretty well, however. The magnetic field required to impart the centripetal acceleration on the particle to keep it in orbit is inversely proportional to the radius of orbit. Creating stronger magnetic fields requires larger and much more costly magnets. Maximising the radius of orbit helps reduce the required strength of magnetic field. Maxwell's equations of electromagnetism indicate that an accelerating charge loses energy by emitting radiation. In fact, if you do the maths, this introduces the limit on the maximum velocity/energy of any accelerated particle. The larger the radius, the lower the centripetal force/acceleration, hence the lower the rate of energy loss. To illustrate point 1, the force on a charged particle is given as $$F = Bqv$$ where B is the magnetic field strength. And the centripetal force on a charge is given as $$F = \frac{mv^2}{r}$$ where r is the radius of the particle accelerator tunnel from the centre of the orbit. Equating the two gives $$B = \frac{mv}{qr} = \frac{m\omega}{q}$$ Clearly, increasing the radius $r$ requires a lower $B$ (magnetic field). Point 2 is slightly more involved to show, as one has to apply Maxwell's equations and calculate the energy flux emitted. Hopefully you get the point however. 
66	physics	 When you accelerate charged particles, they lose energy by emitting photons (a process called "Bremsstrahlung" or "braking radiation"). This is a nuisance in particle accelerators, because (1) you want to impart as much energy as possible to the particles being accelerated (that's the point!) and this is a loss; and (2) the bremsstrahlung can be in the form of harmful (to people and machines) ionizing radiation. Stronger magnets are also necessary to produce the tighter turns. Acceleration in the beam direction is, of course, unavoidable. But any time you have motion along a curved path, you also need acceleration perpendicular to the direction of motion in order to curve the trajectory. In order to minimize this acceleration perpendicular to the direction of motion, it is desirable to have the accelerator be as straight as possible. If you are building a machine that accelerates particles along a closed path, this means you want to make the radius as big as possible. (I assume that, if you do the math, it turns out that a circular path (with uniform curvature) is better than an elliptical one, or one with long straightaways followed by tight turns.) An alternative is to build a linear accelerator, which simply accelerates particles in a straight line. The Stanford Linear Accelerator (SLAC) is one such accelerator; and the International Linear Collider is in the planning phases. Why is the LHC 27 km in circumference? Because they are re-using the tunnels from the Large Electron–Positron Collider (LEP), which was 27 km around. Why was LEP 27 km around? It was almost certainly a balance between the science goals and the money available (longer tunnels = more expensive). Check out the Superconducting Super Collider (SSC) on Wikipedia, the doomed American successor to LEP. It was going to be 87 km in circumference. 
67	physics	 I think that, as always, there is a significant disconnect between the math and physics camps. However, physicists probably realize that the topological partition function of the B-model at genus g is not easy to calculate and doesn't have a good mathematical definition, in the holomorphic limit or not. That is, they would be likely to agree that mathematicians like Costello have done valuable work in trying to nail down this elusive quantity. Physicists working in topological field theory are pretty comfortable with the Atiyah-Segal axioms and with theories of various depths/levels. Kapustin, for example, has spoken widely about these points (categories and 2-categories of branes). 
68	physics	 Our physics teacher showed the class a really interesting demonstration. He used two polarised filters in opposite orientations, then he took some antistatic tape and stretched it under the two plates. The resulting image was projected on the wall using an overhead projector unit. Under stress, the originally clear looking tape (as it looked between the polarised filters) turned all sorts of weird colours, and apparently different colors correlate to different levels of stress. What causes this odd effect? 
69	physics	 The short answer is that measurement and interactions are two different animals in Quantum Mechanics. In reality measurements are performed using one of the fundamental interactions (usually EM), but this does not enter the framework of QM. The long answer is that you will not receive a satisfying answer to your question. First, because physicists don't know the answer, and second, because physicists don't care. Physics is concerned with understanding nature, in so far as making predictions regarding measurements. If we have a theory of what happens between measurements (things like Lagrangians and forces) and a theory of measurements (a postulate in Quantum Mechanics that wave functions collapse, plus the probabilistic interpretation of the abs.square of the wave function), and this framework works to the desired accuracy, then the philosophical implications of trying to unite the two are of no interest to physicists, unless they bring about a deeper understanding of nature, in so far as making more precise or more general predictions regarding measurements. In practice, the line of questioning you pose has been investigated ever since the advent of Quantum Mechanics, but to my knowledge, nothing ever came of it regarding unification of forces and measurements ("don't know"), so the mainstream has lost interest a long time ago ("don't care"). (As an interesting side-note, one important result to come out of related type of inquiry is the Bell inequality.) Sorry if this answer seems negative. To quote David Mermin [corrected] regarding the philosophical issues regarding Quantum Mechanics, the pragmatic thing to do is to "shut up and calculate!" 
70	physics	 The LHC is a synchrotron, that is, a accelerator with a magnetic field confining the orbit on a circular path and using RF accelerating cavities to accelerate the particles. The voltage provided by the cavities is limited (the order of MV) and thus a linear accelerator cannot achieve such high energies (of the order of the TeV) (although some projects of TeV linear collider are in development, CLIC and the ILC) because it would be extremely long. The idea is thus to have a circular path, the particle going through the cavities at each turn and gaining a small amount of energy each turn. To have this circular path, we use a magnetic field, it does not accelerate the particles, but it provides a force perpendicular to the motion, thus allowing to bend the trajectory and to obtain a circular orbit. Why does it have to be that long ? A fundamental relation for the synchrotrons is: p = q B r where p is the particle momentum, q is the charge of the particle, B is the magnetic field and r is the radius of curvature. We can then see that to have a high momentum (and energy) we need a high magnetic field and a large radius. In the LHC, the magnetic field is already at the limit of what a superconducting magnet can achieve (almost 8.5 T). The LHC then needs a very large tunnel. For that it reuses the tunnel of the LEP which was also a synchrotron, but for electrons. In that case the size of the tunnel is not really given by the same reasoning. We need to take into account the synchrotron radiation: any accelerated charge radiates energy in the form of a EM radiation: "light". But the amount of radiation goes with the inverse of the fourth power of the mass, the electron being very light they emit a large amount of radiation. For protons, this effects is almost negligible, that's why it does count for the LHC. But for LEP (and LEP gave it's tunnel to the LHC) this was the main limitation to the achieved energy. And to obtain a high energy, the larger the tunnel the better, because the amount of radiation decreases with the bending angle of the dipole magnets, meaning that a large radius leads to lower radiation. Finally, the size of precisely 27 km was chosen for geographic consideration: the tunnel is between the Jura Mountains and the Leman lake, this implies strict constraints in the civil engineering. 
71	physics	 In laser resonators, higher order modes (i.e. TEM01, etc) accumulate phase faster than the fundamental TEM00 mode. This extra phase is called Gouy phase . What is an intuitive explanation of this effect? Gouy predicted and then experimentally verified the existence of this effect long before the existence of lasers. How did he do it, and what motivated him to think about it? 
72	physics	 Why are protons used in cancer therapy ? Is there any advantages compared to classics X-rays treatment or electron treatment ? 
73	physics	 As a mathematician working the area of representation of Quantum groups, I am constantly thinking about solutions of the Yang-Baxter equation . In particular, trigonometric solutions. Often research grants in this area cite this as an "application" of their research. This being said, many mathematicians (definitely including myself) don't know why these solutions are important. So, I wonder: What exactly do physicists do with solutions to the Yang-Baxter Equation once they have them? 
75	physics	 I'm trying to figure out how to remember that hardness: how resistant it is to deformation toughness: how resistant it is to brittle failures stress: force on a surface area strength: ability to withstand stress without failure strain: measurement of deformation of a material Does anyone know of a mnemonic or easy way? I only know these from googling them, and I'm finding it tricky to remember them. 
77	physics	 The goal of such a treatment is to induce damages in the cells of the tumor by mean of ionizing radiation. These radiations can be X-rays (photons), electron, proton or things like carbon ions. The problem is: if you try to irradiate a tumor, you first have to go through normal tissues and the risk is to damage them also. Photons will transfer energy probabilistically at each collision and the X-ray beam will follow a law like exp[-x] where x is the penetration depth. You then give more energy around the tumor than to the tumor itself. The main advantage of using protons is the "Bragg peak": the way protons interacts with matter, they will give much more energy at a certain depth (th so called Bragg peak), if you tune the energy of the beam, and other parameters, so that the Bragg peak coincide with the tumor, then you have a big advantage. 
78	physics	 I can understand why 2 protons will repel each other, because they're both positive. But there isn't a neutral charge is there? So why do neutrons repel? (Do they, or have I been misinformed?) The reason why I'm asking this is because, I've just been learning about neutron stars and how the neutrons are forced (as in, they repel) together according to my teacher (he's a great teacher btw, though what I just said doesn't make it seem so). So I wondered, why they have to be forced by gravity and not just pushed? 
79	physics	 You can get two photons entangled, and send them off in different directions; this is what happens in EPR experiments. Is the entanglement then somehow affected if one puts a thick slab of EM shielding material between the entangled photons? Have such experiments been made? According to EPR experiments measurements of the entangled states are at odds with SR, so based on that I'd assume the answer is "no"/"don't know", but any citations would be appreciated! 
80	physics	 Neutrons have spin 1/2 and therefore obey the pauli exclusion principle, meaning two neutrons cannot occupy the same space at the same time. When two neutrons' wavefunctions overlap, they feel a strong repulsive force. See . 
81	physics	 As they are not charged, they only interact through their magnetic moment (which at very low energy could lead to kind of Van der Waals forces , the strong force and the weak force. If you try to create a nucleon-nucleon system and that you do some QM calculations using an Hamiltonian taking into account the nuclear force, the electric force and the spin 1/2 of the nucleons you will have different contribution to the total energy, some being positive, other negative. For example, in the proton-proton case you cannot have a bound state because the electric repulsion is stronger. You also have a contribution that depend on the [projection of the] spin of the nucleons. If they are equal the contribution lower the energy of the system and you can have a bound state. That's the case of the deuton (a proton-neutron bound stage, very weak). But in the neutron-neutron state, both being of spin 1/2, you cannot have the same projection, and thus you cannot obtain a bound state. If you interpret that as "they repel each other" then you have your explanation. 
82	physics	 Stress implies changing the thickness, therefore the Bragg reflection change. Or shorter: Interference edit Also, as sigoldberg1 states , Birefringence might occur, too 
83	physics	 Ordinary laser light has equal uncertainty in phase and amplitude. When an otherwise perfect laser beam is incident onto a photodetector, the uncertainty in photon number will produce shot noise with poisson statistics. However, laser light may be transformed into a 'squeezed state', where the uncertainty is no longer equally divided between the two quadratures, resulting in a reduction of shot noise. How is this done? 
84	physics	 Neutrons consist of quarks that are electrically charged, so when two neutrons get close enough to each other the higher electrical multipole moments will become relevant and cause repelling. But also note the magnetic moment and the strong force Cedric mentioned plus the Pauli exclusion mentioned by nibot. 
85	physics	 Neutron mass: 1.008664 u Proton mass: 1.007276 u Why the discrepancy? On a related note, how does one go about measuring the mass of a neutron or proton, anyway? 
86	physics	 I think you answered your question. "According to EPR experiments measurements of the entangled states are at odds with SR": if you mean that we cannot consider that the result of a measurement made on one entangled particle "propagates" to another one because this propagation would violate SR principles, you have to rules out a interaction in the sense of "strong, weak, ... interaction", ie an interaction not in violation with SR. In addition, we do not need to have such an interaction as it is directly explained by the principles of quantum mechanics. It is like imagining that an "interaction" teaches QM to the particles. The main outcome of the treatments of the EPR paradox is to make hidden variables theories irrelevant, so basically "quantum mechanics" wins and we don't need other explanations. 
87	physics	 Masses and coupling between quarks are free parameters in the standard model, so there is not real explanation to that fact. About the measurment: you can have a look at this wikipedia article about Penning traps which are devices used for precision measurements for nucleus. Through the cyclotron frequency (Larmor factor) we can obtain the mass of the particle. Edit: "A neutron is a proton + an electron" is a common answer to this question, but it is a totally invalid reasoning. Both protons and neutrons are made of three quarks. The mass of the quarks is not known with enough precision, and even more important (and that's a why for the masses of the quarks), the interaction between them is responsible for the mass value to a much larger extend. 
88	physics	 Pretty sure that EPR does not state entanglement is at odds with SR or if it does it is incorrect. The point of the EPR paper was that the consequences of entanglement were so strange they could not be real. Experimental evidence however supports entanglement and has never shown any violation of SR. 
89	physics	 By 'Newton's Law of Gravity', I am referring to The magnitude of the force of gravity is proportional to the product of the mass of the two objects and inversely proportional to their distance squared. Does this law of attraction still hold under General Relativity's Tensor Equations? I don't really know enough about mathematics to be able to solve any of Einstein's field equations, but does Newton's basic law of the magnitude of attraction still hold? If they are only approximations, what causes them to differ? 
90	physics	 I'm assuming that what I've been told is true: We can only detect pulsars if their beams of electromagnetic radiation is directed towards Earth. That pulsars are the same as neutron stars, only that they emit beams of EM radiation out of their magnetic poles. So, isn't it possible that neutron stars emit EM radiation in the same fashion as pulsars, just not in the right direction for us to detect it? 
93	physics	 Coulomb's Law states that the fall-off of the strength of the electrostatic force is inversely proportional to the distance squared of the charges. Gauss's law implies that a the total flux through a surface completely enclosing a charge is proportional to the total amount of charge. If we imagine a two-dimensional world of people who knew Gauss's law, they would imagine a surface completely enclosing a charge as a flat circle around the charge. Integrating the flux, they would find that the electrostatic force should be inversely proportional to the distance of the charges, if Gauss's law were true in a two-dimensional world. However, if they observed a 1/r^2 fall-off, this implies a two-dimensional world is not all there is. Is this argument correct? Does the 1/r^2 fall-off imply that there are only three spatial dimensions we live in? I want to make sure this is right before I tell this to my friends and they laugh at me. 
94	physics	 Measuring the polarization of a laser beam is a simple enough task if the polarization is the same everywhere. You can even buy commercial polarimeters. How do you go about it if the light beam has different polarizations in different parts of the transverse plane? One example is a radially polarized beam. More generally, is there a good technique for sampling the local polarization (which might be linear, elliptical, or circular, anywhere on the Poincaré sphere ) in one transverse plane? 
95	physics	 The current "standard model" or concordance model of cosmology is Lambda CDM which includes late time acceleration due to a cosmological constant, cold dark matter as the missing matter component and an inflationary period at very early times. What alternative theories are also consistent with current observational data? 
96	physics	 I would say yes ! Actually some theories explaining quantum gravity use also this reasoning: gravity is a very weak interaction at a quantum level because it "leaks" into other dimensions, not observable at our scale, but that are present at this scale. The mathematical tools are different, but if you just think about gauss's law you can imagine one explanation why additional dimensions are present in these theories. 
97	physics	 Loosely speaking, (super)string theory considers additional spatial dimensions that are "wrapped up" (have unusual topologies of high curvature, I believe). Now it is of course complete speculation, but if these dimensions do exist, electromagnetism would not spread out much into those dimensions, hence it would appear as if there are only three dimensions still (to a very good approximation). Saying that, your argument is more or less sound (though far from bulletproof). It certainly suggests we don't live in a 2D world, and that any possible extra dimensions are comparatively very small! 
98	physics	 For a given isotope, one can obtain the binding energy using the semi-empirical mass formula. For example, has a binding energy of 1782.8 MeV. From this information, how can the likelihood of the isotopes stability be found? 
99	physics	 From the binding energy given experimentally, using precise QM calculations or using a given formula, one should first check for "stability in particles", if the binding is negative, you will of course not have stability. Then the next thing, if you have a formula, is to check for each type of stability. For example, to check for stability against a given fission, calculate the binding energy of the fragments, obtain the new energy and compare. To check for, let's say a beta minus, replace the nucleus (A,Z) in the formula by (A,Z+1), obtain the binding energy, the new total energy (you can safely neglect the mass of the neutrino and even the one of the electron in most cases) and compare. For your specific example, this is a bit tricky because a large variety of decay channels are potentially allowed. Edit Another way to proceed is to look at a binding energy per nucleon or mass excess against A diagram: We can see that 237Np is far on the right as a binding energy per nucleon smaller that the most stable elements like 56Fe. One can then conclude that 237Np can potentially decay to a more stable element to increase it's binding energy (although these decay, that can be alpha decay can have excessively small probability and a time constant excessively long). 
100	physics	 How is it possible to calculate the energy liberated by a given fission process? For example, in the fission of a $^{235}$U induced by capturing a neutron? 
101	physics	 The Tevatron is a proton-antiproton collider; it collides a beam of protons against a beam of antiproton. I can understand how we obtain the protons, but for the antiprotons ? How are they produced ? 
102	physics	 For every force there is an equal force in the opposite direction on another body, correct? So when the Suns gravity acts on Earth where is the opposite and equal force? I also have the same question for centripetal force in a planets orbit. 
103	physics	 The earth feels a force towards the sun. The sun feels an 'equal and opposite' force towards the earth. In fact, the earth does not rotate around the sun; instead, the sun and the earth (if you are considering only those two bodies) orbit around their center of mass. 
104	physics	 Sun and Earth interact through gravity. Sun exert a gravitational force on Earth and ... Earth exert and equal and opposite force on Sun (or is it the opposite ?). Opposite here does not mean "sci-fi anti gravity force repelling objects" but opposite as in "vector of opposite direction and equal magnitude". The centrifugal force is not a force, but a pseudo force that is introduced because the referential frame is not inertial. In the referential frame where you need to introduce the centifugal force, the "opposite reaction" is the centripedal force itself. To solve the Earth around the Sun problem, first consider that the sun as an infinite mass and is thus without motion. The sun exert a force on Earth, and in some frame of reference you need to introduce an additional pseudo-force. Edit: The gravitational field "generated" by Earth is weaker, but the forces are equal (in magnitude). $E_{earth} = \frac{G m_{earth}}{r^2}$ $E_{sun} = \frac{G m_{sun}}{r^2}$ $F = E_{earth} * m_{sun} = E_{sun} * m_{earth}$ Another edit: Considering the title of your question "is gravity a force": Gravity is one of the four fundamental interaction of nature (strong and weak interactions, and electromagnetism). No complete treatment of this interaction exists at a quantum level, so from a classical point of view, yes gravity is a force, associated with a conservative [gravitational] field, etc. 
105	physics	 The opposite is that Earth attracts the sun with the same exact force! Thinking smaller, the Earth attracts you and you attract the earth with forces that are equal in magnitude and direction, but of opposite sense. Of course, by Newton second's law, that same force will have a much greater effect on you than on the Earth. The same applies to the Earth+Sun combination! 
106	physics	 In this simplified picture, gravity of the Sun is a centripetal force for Earth's orbital motion; centrifugal force that equalizes it comes from the angular momentum of Earth. If Earth didn't have an angular momentum (was, say, created near the sun without relative velocity, at least in the angular direction), it would just fall on the Sun. 
108	physics	 When 235U captures a neutron, it forms the compound state 236U*, the excitation energy is given by To find the energy of the compound state, the mass energies of 235U and n can be used - assuming that the neutron will be thermalised for this reaction to occur and thus it's kinetic energy is negligible. Thus: This causes a fission event to occur, which can produce over twenty different products, one example is: The energy liberated is defined by the Q value: the initial mass energy minus the final mass energy Using the masses of 93Rb (92.92172u) and 141Cs (140.91949u) 
109	physics	 Maybe raster scanning the beam in the polarimeters. Or using the same techniques used to measure a single polarization beam but with a camera as a detector. 
1	stats	 How should I elicit prior distributions from experts when fitting a Bayesian model? 
2	stats	 In many different statistical methods there is an "assumption of normality". What is "normality" and how do I know if there is normality? 
3	stats	 What are some valuable Statistical Analysis open source projects available right now? Edit: as pointed out by Sharpie, valuable could mean helping you get things done faster or more cheaply. 
4	stats	 I have two groups of data. Each with a different distribution of multiple variables. I'm trying to determine if these two groups' distributions are different in a statistically significant way. I have the data in both raw form and binned up in easier to deal with discrete categories with frequency counts in each. What tests/procedures/methods should I use to determine whether or not these two groups are significantly different and how do I do that in SAS or R (or Orange)? 
5	stats	 The R-project R is valuable and significant because it was the first widely-accepted Open-Source alternative to big-box packages. It's mature, well supported, and a standard within many scientific communities. Some reasons why it is useful and valuable There are some nice tutorials here . 
6	stats	 Last year, I read a blog post from Brendan O'Connor entitled "Statistics vs. Machine Learning, fight!" that discussed some of the differences between the two fields. Andrew Gelman responded favorably to this : Simon Blomberg: From R's fortunes package: To paraphrase provocatively, 'machine learning is statistics minus any checking of models and assumptions'. -- Brian D. Ripley (about the difference between machine learning and statistics) useR! 2004, Vienna (May 2004) :-) Season's Greetings! Andrew Gelman: In that case, maybe we should get rid of checking of models and assumptions more often. Then maybe we'd be able to solve some of the problems that the machine learning people can solve but we can't! There was also the "Statistical Modeling: The Two Cultures" paper by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the predictive accuracy of models. Has the statistics field changed over the last decade in response to these critiques? Do the two cultures still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines? 
7	stats	 I've been working on a new method for analyzing and parsing datasets to identify and isolate subgroups of a population without foreknowledge of any subgroup's characteristics. While the method works well enough with artificial data samples (i.e. datasets created specifically for the purpose of identifying and segregating subsets of the population), I'd like to try testing it with live data. What I'm looking for is a freely available (i.e. non-confidential, non-proprietary) data source. Preferably one containing bimodal or multimodal distributions or being obviously comprised of multiple subsets that cannot be easily pulled apart via traditional means. Where would I go to find such information? 
8	stats	 Sorry, but the emptyness was a bit overwhelming. And this has been stuck in my head since it got asked at Area51! 
9	stats	 Incanter is a Clojure-based, R-like platform (environment + libraries) for statistical computing and graphics. 
10	stats	 Many studies in the social sciences use Likert scales. When is it appropriate to use Likert data as ordinal and when is it appropriate to use it as interval data? 
11	stats	 Is there a good, modern treatment covering the various methods of multivariate interpolation, including which methodologies are typically best for particular types of problems? I'm interested in a solid statistical treatment including error estimates under various model assumptions. An example: Shepard's method Say we're sampling from a multivariate normal distribution with unknown parameters. What can we say about the standard error of the interpolated estimates? I was hoping for a pointer to a general survey addressing similar questions for the various types of multivariate interpolations in common use. 
12	stats	 See my response to "Datasets for Running Statistical Analysis on" in reference to datasets in R. 
13	stats	 Machine Learning seems to have its basis in the pragmatic - a Practical observation or simulation of reality. Even within statistics, mindless "checking of models and assumptions" can lead to discarding methods that are useful. For example, years ago, the very first commercially available (and working) Bankruptcy model implemented by the credit bureaus was created through a plain old linear regression model targeting a 0-1 outcome. Technically, that's a bad approach, but practically, it worked. 
14	stats	 I second that Jay. Why is R valuable? Here's a short list of reasons. . Also check out ggplot2 - a very nice graphics package for R. Some nice tutorials here . 
15	stats	 John Cook gives some interesting recommendations. Basically, get percentiles/quantiles (not means or obscure scale parameters!) from the experts, and fit them with the appropriate distribution. 
16	stats	 Two projects spring to mind: Bugs - taking (some of) the pain out of Bayesian statistics. It allows the user to focus more on the model and a bit less on MCMC. Bioconductor - perhaps the most popular statistical tool in Bioinformatics. I know it's a R repository, but there are a large number of people who want to learn R, just for Bioconductor. The number of packages available for cutting edge analysis, make it second to none. 
17	stats	 I have four competing models which I use to predict a binary outcome variable (say, employment status after graduating, 1 = employed, 0 = not-employed) for n subjects. A natural metric of model performance is hit rate which is the percentage of correct predictions for each one of the models. It seems to me that I cannot use ANOVA in this setting as the data violates the assumptions underlying ANOVA. Is there an equivalent procedure I could use instead of ANOVA in the above setting to test for the hypothesis that all four models are equally effective? 
18	stats	 Also see the UCI machine learning Data Repository. 
19	stats	 Gapminder has a number (430 at the last look) of datasets, which may or may not be of use to you. 
20	stats	 The assumption of normality assumes your data is normally distributed (the bell curve, or gaussian distribution). You can check this by plotting the data or checking the measures for kurtosis (how sharp the peak is) and skewdness (?) (if more than half the data is on one side of the peak). 
21	stats	 What are some of the ways to forecast demographic census with some validation and calibration techniques? Some of the concerns: Census blocks vary in sizes as rural areas are a lot larger than condensed urban areas. Is there a need to account for the area size difference? if let's say I have census data dating back to 4 - 5 census periods, how far can i forecast it into the future? if some of the census zone change lightly in boundaries, how can i account for that change? What are the methods to validate census forecasts? for example, if i have data for existing 5 census periods, should I model the first 3 and test it on the latter two? or is there another way? what's the state of practice in forecasting census data, and what are some of the state of the art methods? 
22	stats	 How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning? 
23	stats	 How can I find the PDF (probability density function) of a distribution given the CDF (cumulative distribution function)? 
24	stats	 For doing a variety of MCMC tasks in Python, there's PyMC , which I've gotten quite a bit of use out of. I haven't run across anything that I can do in BUGS that I can't do in PyMC, and the way you specify models and bring in data seems to be a lot more intuitive to me. 
25	stats	 What modern tools (Windows-based) do you suggest for modeling financial time series? 
26	stats	 What is a standard deviation, how is it calculated and what is its use in statistics? 
27	stats	 
28	stats	 GSL for those of you who wish to program in C / C++ is a valuable resource as it provides several routines for random generators, linear algebra etc. While GSL is primarily available for Linux there are also ports for Windows. (See: and ) 
29	stats	 Contingency table (chi-square). Also Logistic Regression is your friend - use dummy variables. 
30	stats	 Which methods are used for testing random variate generation algorithms? 
31	stats	 After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests. It seems that students easily learn how to perform the calculations required by a given test but get hung up on interpreting the results. Many computerized tools report test results in terms of "p values" or "t values". How would you explain the following points to college students taking their first course in statistics: What does a "p-value" mean in relation to the hypothesis being tested? Are there cases when one should be looking for a high p-value or a low p-value? What is the relationship between a p-value and a t-value? 
32	stats	 I recommend R (see the time series view on CRAN ). Some useful references: Econometrics in R , by Grant Farnsworth Multivariate time series modelling in R 
33	stats	 What R packages should I install for seasonality analysis? 
35	stats	 I have a data set that I'd expect to follow a Poisson distribution, but it is overdispersed by about 3-fold. At the present, I'm modelling this overdispersion using something like the following code in R. Visually, this seems to fit my empirical data very well. If I'm happy with the fit, is there any reason that I should be doing something more complex, like using a negative binomial distribution, as described here ? (If so, any pointers or links on doing so would be much appreciated). Oh, and I'm aware that this creates a slightly jagged distribution (due to the multiplication by three), but that shouldn't matter for my application. Update: For the sake of anyone else who searches and finds this question, here's a simple R function to model an overdispersed poisson using a negative binomial distribution. Set d to the desired mean/variance ratio: (via the R mailing list: https://stat.ethz.ch/pipermail/r-help/2002-June/022425.html ) 
36	stats	 There is an old saying: "Correlation does not mean causation". When I teach, I tend to use the following standard examples to illustrate this point: number of storks and birth rate in Denmark; number of priests in America and alcoholism; in the start of the 20th century it was noted that there was a strong correlation between 'Number of radios' and 'Number of people in Insane Asylums' and my favorite: pirates cause global warming . However, I do not have any references for these examples and whilst amusing, they are obviously false. Does anyone have any other good examples? 
38	stats	 If your mean value for the Poisson is 1500, then you're very close to a normal distribution; you might try using that as an approximation and then modelling the mean and variance separately. 
39	stats	 I'm looking for worked out solutions using Bayesian and/or logit analysis similar to a workbook or an annal. The worked out problems could be of any field; however, I'm interested in urban planning / transportation related fields. 
40	stats	 What algorithms are used in modern and good-quality random number generators? 
41	stats	 A quote from Wikipedia . It shows how much variation there is from the "average" (mean, or expected/budgeted value). A low standard deviation indicates that the data points tend to be very close to the mean, whereas high standard deviation indicates that the data is spread out over a large range of values. 
42	stats	 Weka for data mining - contains many classification and clustering algorithms in Java. 
43	stats	 R is great, but I wouldn't really call it "windows based" :) That's like saying the cmd prompt is windows based. I guess it is technically in a window... RapidMiner is far easier to use [1]. It's a free, open-source, multi-platform, GUI. Here's a video on time series forecasting: Also, don't forget to read: [1] No, I don't work for them. 
44	stats	 How would you explain data visualization and why it is important to a layman? 
45	stats	 The Mersenne Twister is one I've come across and used before now. 
46	stats	 A standard deviation is the square root of the second central moment of a distribution. A central moment is the expected difference from the expected value of the distribution. A first central moment would usually be 0, so we define a second central moment as the expected value of the squared distance of a random variable from its expected value. To put it on a scale that is more in line with the original observations, we take the square root of that second central moment and call it the standard deviation. Standard deviation is a property of a population. It measures how much average "dispersion" there is to that population. Are all the obsrvations clustered around the mean, or are they widely spread out? To estimate the standard deviation of a population, we often calculate the standard deviation of a "sample" from that population. To do this, you take observations from that population, calculate a mean of those observations, and then calculate the square root of the average squared deviation from that "sample mean". To get an unbiased estimator of the variance, you don't actually calculate the average squared deviation from the sample mean, but instead, you divide by (N-1) where N is the number of observations in your sample. Note that this "sample standard deviation" is not an unbiased estimator of the standard deviation, but the square of the "sample standard deviation" is an unbiased estimator of the variance of the population. 
47	stats	 I have a dataset of 130k internet users characterized by 4 variables describing users' number of sessions, locations visited, avg data download and session time aggregated from four months of activity. Dataset is very heavy-tailed. For example third of users logged only once during four months, whereas six users had more than 1000 sessions. I wanted to come up with a simple classification of users, preferably with indication of the most appropriate number of clusters. Is there anything you could recomend as a soultion? 
49	stats	 You don't need to install any packages because this is possible with base-R functions. Have a look at the arima function . This is a basic function of Box-Jenkins analysis , so you should consider reading one of the R time series text-books for an overview; my favorite is Shumway and Stoffer. " Time Series Analysis and Its Applications: With R Examples ". 
50	stats	 What do they mean when they say "random variable"? 
51	stats	 Are there any objective methods of assessment or standardized tests available to measure the effectiveness of a software that does pattern recognition? 
52	stats	 Usually one (p=.042), but it also depends on power. 
53	stats	 What are the main differences between performing principal component analysis (PCA) on the correlation matrix and on the covariance matrix? Do they give the same results? 
54	stats	 As I understand UK Schools teach that the Standard Deviation is found using: whereas US Schools teach: (at a basic level anyway). This has caused a number of my students problems in the past as they have searched on the Internet, but found the wrong explanation. Why the difference? With simple datasets say 10 values, what degree of error will there be if the wrong method is applied (eg in an exam)? 
55	stats	 The Diehard Test Suite is something close to a Golden Standard for testing random number generators. It includes a number of tests where a good random number generator should produce result distributed according to some know distribution against which the outcome using the tested generator can then be compared. EDIT I have to update this since I was not exactly right: Diehard might still be used a lot, but it is no longer maintained and not state-of-the-art anymore. NIST has come up with a set of improved tests since. 
56	stats	 Here is how I would explain the basic difference to my grandma: I have misplaced my phone somewhere in the home. I can use the phone locator on the base of the instrument to locate the phone and when I press the phone locator the phone starts beeping. Problem: Which area of my home should I search? Frequentist Reasoning I can hear the phone beeping. I also have a mental model which helps me identify the area from which the sound is coming. Therefore, upon hearing the beep, I infer the area of my home I must search to locate the phone. Bayesian Reasoning I can hear the phone beeping. Now, apart from a mental model which helps me identify the area from which the sound is coming from, I also know the locations where I have misplaced the phone in the past. So, I combine my inferences using the beeps and my prior information about the locations I have misplaced the phone in the past to identify an area I must search to locate the phone. 
57	stats	 From Wikipedia : In mathematics (especially probability theory and statistics), a random variable (or stochastic variable) is (in general) a measurable function that maps a probability space into a measurable space. Random variables mapping all possible outcomes of an event into the real numbers are frequently studied in elementary statistics and used in the sciences to make predictions based on data obtained from scientific experiments. In addition to scientific applications, random variables were developed for the analysis of games of chance and stochastic events. The utility of random variables comes from their ability to capture only the mathematical properties necessary to answer probabilistic questions. From cnx.org : A random variable is a function, which assigns unique numerical values to all possible outcomes of a random experiment under fixed conditions. A random variable is not a variable but rather a function that maps events to numbers. 
58	stats	 What is the back-propagation algorithm and how does it work? 
59	stats	 The assumption of normality is just the supposition that the underlying random variable of interest is distributed normally , or approximately so. Intuitively, normality may be understood as the result of the sum of a large number of independent random events. More specifically, normal distributions are defined by the following function: where $\mu$ and $\sigma^2$ are the mean and the variance, respectively, and which appears as follows: This can be checked in multiple ways , that may be more or less suited to your problem by its features, such as the size of n. Basically, they all test for features expected if the distribution were normal (e.g. expected quantile distribution ). 
60	stats	 K-Means clustering should work well for this type of problem. However, it does require that you specify the number of clusters in advance. Given the nature of this data, however, you may be able to work with a hierarchical clustering algorithm instead. Since all 4 variables are most likely fairly highly correlated, you can most likely break out clusters, and stop when you reach a small enough distance between clusters. This may be a much simpler approach in this specific case, and allows you to determine "how many clusters" by just stopping as soon as you've broken your set into fine enough clusters. 
61	stats	 Standard deviation is a number that represents the "spread" or "dispersion" of a set of data. There are other measures for spread, such as range and variance. Here are some example sets of data, and their standard deviations: The above data sets have the same mean. Deviation means "distance from the mean". "Standard" here means "standardized", meaning the standard deviation and mean are in the same units, unlike variance. For example, if the mean height is 2 meters , the standard deviation might be 0.3 meters , whereas the variance would be 0.09 meters squared . It is convenient to know that at least 75% of the data points always lie within 2 standard deviations of the mean (or around 95% if the distribution is Normal). For example, if the mean is 100, and the standard deviation is 15, then at least 75% of the values are between 70 and 130. If the distribution happens to be Normal, then 95% of the values are between 70 and 130. Generally speaking, IQ test scores are normally distributed and have an average of 100. Someone who is "very bright" is two standard deviations above the mean, meaning an IQ test score of 130. 
62	stats	 With the recent FIFA world cup, I decided to have some fun and determine which months produced world cup football players. Turned out, most footballers in the 2010 world cup were born in the first half of the year. Someone pointed out, that children born in the first half of the year had a physical advantage over others and hence "survivorship bias" was involved in the equation. Is this an accurate observation? Can someone please explain why he says that? Also, when trying to understand the concept, I found most examples revolved around the financial sector. Are they any other everyday life examples explaining it? Thanks! 
63	stats	 It might be useful to explain that "causes" is an asymmetric relation (X causes Y is different from Y causes X), whereas "is correlated with" is a symmetric relation. For instance, homeless population and crime rate might be correlated, in that both tend to be high or low in the same locations. It is equally valid to say that homelesss population is correlated with crime rate, or crime rate is correlated with homeless population. To say that crime causes homelessness, or homeless populations cause crime are different statements. And correlation does not imply that either is true. For instance, the underlying cause could be a 3rd variable such as drug abuse, or unemployment. The mathematics of statistics is not good at identifying underlying causes, which requires some other form of judgement. 
64	stats	 Yes, there are many methods. You would need to specify which model you're using, because it can vary. For instance, Some models will be compared based on the AIC or BIC criteria. In other cases, one would look at the MSE from cross validation (as, for instance, with a support vector machine). I recommend reading Pattern Recognition and Machine Learning by Christopher Bishop. This is also discussed in Chapter 5 on Credibility, and particularly section 5.5 "Comparing data mining methods" of Data Mining: Practical Machine Learning Tools and Techniques by Witten and Frank (which discusses Weka in detail). Lastly, you should also have a look at The Elements of Statistical Learning by Hastie, Tibshirani and Friedman which is available for free online. 
65	stats	 The first formula is the population standard deviation and the second formula is the the sample standard deviation. The second formula is also related to the unbiased estimator of the variance - see wikipedia for further details. I suppose (here) in the UK they don't make the distinction between sample and population at high school. They certainly don't touch concepts such as biased estimators. 
66	stats	 This is Bessel's Correction . The US version is showing the formula for the sample standard deviation , where the UK version above is the standard deviation of the sample . 
67	stats	 From Wikipedia : Data visualization is the study of the visual representation of data, meaning "information which has been abstracted in some schematic form, including attributes or variables for the units of information" Data viz is important for visualizing trends in data, telling a story - See Minard's map of Napoleon's march - possibly one of the best data graphics ever printed. Also see any of Edward Tufte's books - especially Visual Display of Quantitative Information. 
69	stats	 Since N is the number of points in the data set, one could argue that by calculating the mean one has reduced the degree of freedom in the data set by one (since one introduced a dependency into the data set), so one should use N-1 when estimating the standard deviation from a data set for which one had to estimate the mean before. 
70	stats	 World Bank offers quite a lot of interesting data and has been recently very active in developing nice API for it. Also, commugrate project has an interesting list available. For US health related data head for Health Indicators Warehouse . Daniel Lemire's blog points to few interesting examples (mostly tailored towards DB research) including Canadian Census 1880 and synoptic cloud reports . And as for today (03/04/2012) US 1940 census records are also available to download. 
71	stats	 It's an algorithm for training feedforward multilayer neural networks (multilayer perceptrons). There are several nice java applets around the web that illustrate what's happening, like this one: . Also, Bishop's book on NNs is the standard desk reference for anything to do with NNs. 
72	stats	 for overdispersed poisson, use the negative binomial, which allows you to parameterize the variance as a function of the mean precisely. rnbinom(), etc. in R. 
73	stats	 Duplicate thread: I just installed the latest version of R. What packages should I obtain? What are the R packages you couldn't imagine your daily work with data? Please list both general and specific tools. UPDATE: As for 24.10.10 seems to be the winer with 7 votes. Other packages mentioned more than one are: - 4 , - 4 - 3 - 2 - 2 - 2 - 2 - 2 Thanks all for your answers! 
74	stats	 In such a discussion, I always recall the famous Ken Thompson quote When in doubt, use brute force. In this case, machine learning is a salvation when the assumptions are hard to catch; or at least it is much better than guessing them wrong. 
75	stats	 I'm using R and the manuals on the R site are really informative. However, I'd like to see some more examples and implementations with R which can help me develop my knowledge faster. Any suggestions? 
76	stats	 I use plyr and ggplot2 the most on a daily basis. I also rely heavily on time series packages; most especially, the zoo package. 
77	stats	 Sometimes correlation is enough. For example, in car insurance, male drivers are correlated with more accidents, so insurance companies charge them more. There is no way you could actually test this for causation. You cannot change the genders of the drivers experimentally. Google has made hundreds of billions of dollars not caring about causation. To find causation, you generally need experimental data, not observational data. Though, in economics, they often use observed "shocks" to the system to test for causation, like if a CEO dies suddenly and the stock price goes up, you can assume causation. Correlation is a necessary but not sufficient condition for causation. To show causation requires a counter-factual. 
78	stats	 You tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales. Using the correlation matrix standardises the data. In general they give different results. Especially when the scales are different. As an example, take a look at this R data set. Some of the variables have an average value of about 1.8 (the high jump), whereas other variables (run 800m) are around 120. This outputs: Now let's do PCA on covariance and on correlation: Notice that PCA on covariance is dominated by and : PC1 is almost equal to (and explains $82\%$ of the variance) and PC2 is almost equal to (together they explain $97\%$). PCA on correlation is much more informative and reveals some structure in the data and relationships between variables (but note that the explained variances drop to $64\%$ and $71\%$). Notice also that the outlying individuals (in this data set) are outliers regardless of whether the covariance or correlation matrix is used. 
79	stats	 I am not sure this is purely a US vs. British issue. Here is a brief page I wrote explaining the difference between using n vs. n-1 when computing a Standard Deviation . 
80	stats	 The basic idea behind this is that football clubs have an age cut-off when determining teams. In the league my children participate in the age restrictions states that children born after July 31st are placed on the younger team. This means that two children that are effectively the same age can be playing with two different age groups. The child born July 31st will be playing on the older team and theoretically be the youngest and smallest on the team and in the league. The child born on August 1st will be the oldest and largest child in the league and will be able to benefit from that. The survivorship bias comes because competitive leagues will select the best players for their teams. The best players in childhood are often the older players since they have additional time for their bodies to mature. This means that otherwise acceptable younger players are not selected simply because of their age. Since they are not given the same opportunities as the older kids, they don’t develop the same skills and eventually drop out of competitive soccer. If the cut-off for competitive soccer in enough countries is January 1st, that would support the phenomena you see. A similar phenomena has been observed in several other sports including baseball and ice hockey. 
81	stats	 I use the xtable package. The xtable package turns tables produced by R (in particular, the tables displaying the anova results) into LaTeX tables, to be included in an article. 
83	stats	 R is designed around ideas such as "reproducible research" and "trustworthy software", as John Chambers says in his excellent book "Software for Data Analysis: Programming with R" . One of the best ways to learn R is to look at the wealth of source code that available on CRAN (with 2461 packages and counting). Simple , load a , and start browsing the code. 
84	stats	 I would explain it to a layman as: Data visualization is taking data, and making a picture out of it. This allows you to easily see and understand relationships within the data much more easily than just looking at the numbers. 
85	stats	 A random variable is a variable whose value depends on unknown events. We can summarize the unknown events as "state", and then the random variable is a function of the state. Example: Suppose we have three dice rolls (D1,D2,D3). Then the state S=(D1,D2,D3). One random variable X is the number of 5s. X=(D1==5?)+(D2==5?)+(D3==5?). Another random variable Y is the sum of the dice rolls Y=D1+D2+D3. 
86	stats	 Unlike a regular variable, a random variable may not be substituted for a single, unchanging value. Rather statistical properties such as the distribution of the random variable may be stated. The distribution is a function that provides the probability the variable will take on a given value, or fall within a range given certain parameters such as the mean or standard deviation. Random variables may be classified as discrete if the distribution describes values from a countable set, such as the integers. The other classification for a random variable is continuous and is used if the distribution covers values from an uncountable set such as the real numbers. 
89	stats	 When I teach very basic statistics to Secondary School Students I talk about evolution and how we have evolved to spot patterns in pictures rather than lists of numbers and that data visualisation is one of the techniques we use to take advantage of this fact. Plus I try to talk about recent news stories where statistical insight contradicts what the press is implying, making use of sites like Gapminder to find the representation before choosing the story. 
90	stats	 R bloggers has been steadily supplying me with a lot of good pragmatic content. From the author: 
91	stats	 As user28 said in comments above, the pdf is the first derivative of the cdf for a continuous random variable, and the difference for a discrete random variable. In the continuous case, wherever the cdf has a discontinuity the pdf has an atom. Dirac delta "functions" can be used to represent these atoms. 
93	stats	 We're trying to use a Gaussian process to model h(t) -- the hazard function -- for a very small initial population, and then fit that using the available data. While this gives us nice plots for credible sets for h(t) and so on, it unfortunately is also just pushing the inference problem from h(t) to the covariance function of our process. Perhaps predictably, we have several reasonable and equally defensible guesses for this that all produce different result. Has anyone run across any good approaches for addressing such a problem? Gaussian-process related or otherwise? 
94	stats	 Quick R site is basic, but quite nice for start . 
95	stats	 I have been using various GARCH-based models to forecast volatility for various North American equities using historical daily data as inputs. Asymmetric GARCH models are often cited as a modification of the basic GARCH model to account for the 'leverage effect' i.e. volatility tends to increase more after a negative return than a similarly sized positive return. What kind of a difference would you expect to see between a standard GARCH and an asymmetric GARCH forecast for a broad-based equity index like the S&amp;P 500 or the NASDAQ-100? There is nothing particularly special about these two indices, but I think it is helpful to give something concrete to focus the discussion, as I am sure the effect would be different depending on the equities used. 
96	stats	 Another great resource is the LearnR blog , which went through an extensive study of visualizations with lattice and ggplot2. 
97	stats	 I have some ordinal data gained from survey questions. In my case they are Likert style responses (Strongly Disagree-Disagree-Neutral-Agree-Strongly Agree). In my data they are coded as 1-5. I don't think means would mean much here, so what basic summary statistics are considered usefull? 
98	stats	 Eliciting priors is a tricky business. Statistical Methods for Eliciting Probability Distributions and Eliciting Probability Distributions are quite good practical guides for prior elicitation. The process in both papers is outlined as follows: background and preparation; identifying and recruiting the expert(s); motivation and training the expert(s); structuring and decomposition (typically deciding precisely what variables should be elicited, and how to elicit joint distributions in the multivariate case); the elicitation itself. Of course, they also review how the elicitation results in information that may be fit to or otherwise define distributions (for instance, in the Bayesian context, Beta distributions ), but quite importantly, they also address common pitfalls in modeling expert knowledge (anchoring, narrow and small-tailed distributions, etc.). 
99	stats	 multicore is quite nice for tool for making faster scripts faster. cacheSweave saves a lot of time when using . 
100	stats	 I'd like to see the answer with qualitative view on the problem, not just definition. Examples and analogous from other areas of applied math also would be good. I understand, my question is silly, but I can't find good and intuitive introduction textbook on signal processing — if someone would suggest one, I will be happy. Thanks. 
101	stats	 Understanding $p$-value Suppose, that you want to test the hypothesis that the average height of male students at your University is $5$ ft $7$ inches. You collect heights of $100$ students selected at random and compute the sample mean (say it turns out to be $5$ ft $9$ inches). Using an appropriate formula/statistical routine you compute the $p$-value for your hypothesis and say it turns out to be $0.06$. In order to interpret $p=0.06$ appropriately, we should keep several things in mind: The first step under classical hypothesis testing is the assumption that the hypothesis under consideration is true. (In our context, we assume that the true average height is $5$ ft $7$ inches.) Imagine doing the following calculation: Compute the probability that the sample mean is greater than $5$ ft $9$ inches assuming that our hypothesis is in fact correct (see point 1). In other words, we want to know $$\mathrm{P}(\mathrm{Sample\: mean} \ge 5 \:\mathrm{ft} \:9 \:\mathrm{inches} \:|\: \mathrm{True\: value} = 5 \:\mathrm{ft}\: 7\: \mathrm{inches}).$$ The calculation in step 2 is what is called the $p$-value. Therefore, a $p$-value of $0.06$ would mean that if we were to repeat our experiment many, many times (each time we select $100$ students at random and compute the sample mean) then $6$ times out of $100$ we can expect to see a sample mean greater than or equal to $5$ ft $9$ inches. Given the above understanding, should we still retain our assumption that our hypothesis is true (see step 1)? Well, a $p=0.06$ indicates that one of two things have happened: (A) Either our hypothesis is correct and an extremely unlikely event has occurred (e.g., all $100$ students are student athletes) or (B) Our assumption is incorrect and the sample we have obtained is not that unusual. The traditional way to choose between (A) and (B) is to choose an arbitrary cut-off for $p$. We choose (A) if $p &gt; 0.05$ and (B) if $p &lt; 0.05$. 
102	stats	 For me Illuminating the Path report has been always good point of reference. For more recent overview you can also have a look at good article by Heer and colleagues. But what would explain better than visualization itself? ( Source ) 
103	stats	 What is the best blog on data visualization? I'm making this question a community wiki since it is highly subjective. Please limit each answer to one link. Please note the following criteria for proposed answers: [A]cceptable answers to questions like this ...need to supply adequate descriptions and reasoned justification. A mere hyperlink doesn't do it. ...[A]ny future replies [must] meet ...[these] standards; otherwise, they will be deleted without further comment . 
104	stats	 A frequency table is a good place to start. You can do the count, and relative frequency for each level. Also, the total count, and number of missing values may be of use. You can also use a contingency table to compare two variables at once. Can display using a mosaic plot too. 
105	stats	 FlowingData | Data Visualization, Infographics, and Statistics 
107	stats	 information aesthetics - Data Visualization &amp; Information Design 
108	stats	 Information Is Beautiful | Ideas, issues, knowledge, data - visualized! 
109	stats	 Following one-way ANOVA, there are many possible follow-up multiple comparison tests. Holm's test (or better, the Holm-Sidak) test has lots of power, but because it works in a stepwise manner, it cannot compute confidence intervals. Its advantage over the tests than can compute confidence intervals (Tukey, Dunnett) is that is has more power. But is it fair to say that the Holm method always has more power than the methods of Tukey and Dunnet? Or does it depend...? 
