110	physics	 I generally tell the story that the action principle is another way of getting at the same differential equations -- so at the level of mechanics, the two are equivalent. However, when it comes to quantum field theory, the description in terms of path integrals over the exponentiated action is essential when considering instanton effects. So eventually one finds that the formulation in terms of actions is more fundamental, and more physically sound. But still, people don't have a "feel" for action the way they have a feel for energy. 
111	physics	 I am looking for a book about "advanced" classical mechanics. By advanced I mean a book considering directly Lagrangian and Hamiltonian formulation, and also providing a firm basis in the geometrical consideration related to these to formalism (like tangent bundle, cotangent bundle, 1-form, 2-form, etc.). I have this book from Saletan and Jose , but I would like to go into more details about the [symplectic] geometrical and mathematical foundations of classical mechanics. Additional note: A chapter about relativistic Hamiltonian dynamics would be a good thing. 
112	physics	 Yes, in the appropriate limit. Roughly, the study of geodesic motion in the Schwarzschild solution (which is radially symmetric) reduces to Newtonian gravity at sufficiently large distances and slow speeds. To see how this works exactly, one must look more specifically at the equations. 
113	physics	 Maybe no symplectic geometry or forms here, but this book has a LOT to offer: 
114	physics	 What happens if you lean on bike while trying to ride straight? It happens that you turn on same side you lean. Why, because you have to generate centrifugal force and lean back up if you do not want to fall down. So if you lean left you have to make left turn in order to generate force which will turn you straight position. If you turn too much left you will eventually fall on right because of too much centrifugal force kicking you other side. 
115	physics	 I have an Hamiltonian problem whose 2D phase space exhibit islands of stability (elliptic fixed points). I can calculate the area of these islands in some cases, but for other cases I would like to use Mathematica (or anything else) to compute it numerically. The phase space looks like that : This is a contour plot make with Mathematica. Could anyone with some knowledge of Mathematica provide a way to achieve this ? 
116	physics	 Structure and Interpretation of Classical Mechanics ( table of contents ) certainly deserves mention. It might not have as much differential geometry as you'd like, though they have a followup article titled Functional Differential Geometry . 
117	physics	 I think it is correct your appreciation if Gauss law holds in a two dimensional world, then the electrostatic force should be inversely proportional to the distance between charges. However, I'm not at all convinced that Gauss law could be true in a two-dimensional world because $F_{q}=k \displaystyle \frac{qq'(r-r')}{|r-r'|^{3}}$ is a consequence of a 3-dimensional space and since we derive Gauss law from such a force law (to be precise, from its electric field), we can not assume the validity of Gauss law independently from a 3-dimensional space. 
118	physics	 Pure beaten gold, any edition, paperback. It never leaves you. Lagrangian approach. See the reviews on Amazon L D Landau (Author), E.M. Lifshitz (Author) 
119	physics	 There is also Feynman's approach, i.e. least action is true classically just because it is true quantum mechanically, and classical physics is best considered as an approximation to the underlying quantum approach. See or l . Basically, the whole thing is summarized in a nutshell in Richard P. Feynman, The Feynman Lectures on Physics (Addison–Wesley, Reading, MA, 1964), Vol. II, Chap. 19. (I think, please correct me if I'm wrong here). The fundamental idea is that the action integral defines the quantum mechanical amplitude for the position of the particle, and the amplitude is stable to interference effects (--&gt;has nonzero probability of occurrence) only at extrema or saddle points of the action integral. The particle really does explore all alternative paths probabilistically. You likely want to read Feynman's Lectures on Physics anyway, so you might as well start now. :-) 
120	physics	 Yes, absolutely. In fact, Gauss's law is generally considered to be the fundamental law, and Coulomb's law is simply a consequence of it (and of the Lorentz force law). You can actually simulate a 2D world by using a line charge instead of a point charge, and taking a cross section perpendicular to the line. In this case, you find that the force (or electric field) is proportional to 1/r, not 1/r^2, so Gauss's law is still perfectly valid. I believe the same conclusion can be made from experiments performed in graphene sheets and the like, which are even better simulations of a true 2D universe, but I don't know of a specific reference to cite for that. 
121	physics	 Squeezed light can be generated from light in a coherent state or vacuum state by using certain optical nonlinear interactions. For example, an optical parametric amplifier with a vacuum input can generate a squeezed vacuum with a reduction in the noise of one quadrature components by the order of 10 dB. A lower degree of squeezing in bright amplitude-squeezed light can under some circumstances be obtained with frequency doubling. Squeezing can also arise from atom-light interactions. References: 
122	physics	 I learned today in class that photons and light are quantized. I also remember that electric charge is quantized as well. I was thinking about these implications, and I was wondering if (rest) mass was similarly quantized. That is, if we describe certain finite irreducible masses $x$, $y$, $z$, etc., then all masses are integer multiples of these irreducible masses. Or do masses exist along a continuum, as charge and light were thought to exist on before the discovery of photons and electrons? (I'm only referring to invariant/rest mass.) 
123	physics	 Trained as a pure mathematician, I see claims about the mass of a galaxy and other such huge measurements that are arrived at experimentally, and I just have to scratch my head. I know this is a bit of a vague question--but does anyone have a good resource for something like "measurement in astrophysics" or an introductory history of how astrophysicists figured out how to make measurements of this kind? 
124	physics	 Rest mass of elementary particles is not quantized: in the standard model, the masses are free parameters of the theory; they must be measured and introduced in the model experimentally. However, the mass of, say the Hydrogen atom is given by the mass of its constituent (proton and electron whose mass are given) minus the binding energy which is quantized. 
125	physics	 There are a couple different meanings of the word that you should be aware of: In popular usage, "quantized" means that something only ever occurs in integer multiples of a certain unit, or a sum of integer multiples of a few units, usually because you have an integer number of objects each of which carries that unit. This is the sense in which charge is quantized. In technical usage, "quantized" means being limited to certain discrete values, namely the eigenvalues of an operator, although those discrete values will not necessarily be multiples of a certain unit. As far as we know, mass is not quantized in either of these ways... mostly. But let's leave that aside for a moment. For fundamental particles (those which are not known to be composite), we have tabulated the masses, and they are clearly not multiples of a single unit. So that rules out the first meaning of quantization. As for the second, there is no known operator whose eigenvalues correspond to (or even are proportional to) the masses of the fundamental particles. Many physicists suspect that such an operator exists and that we will find it someday, but so far there is no evidence for it, and in fact there is basically no concrete evidence that the masses of the fundamental particles have any particular significance. This is why I would not say that mass is quantized. When you consider composite particles, though, things get a little trickier. Much of their mass comes from the kinetic energy and binding energy of the constituents, not from the masses of the constituents themselves. For instance, only a small part of the mass of the proton comes from the masses of its quarks. Most of the proton's mass is actually the kinetic energy of the quarks and gluons. These particles are moving around inside the proton even when the proton itself is at rest, so their energy of motion contributes to the rest mass of the proton. There is also a contribution from the potential energy that all the constituents of the proton have by virtue of being subject to the strong force. This contribution, the binding energy, is actually negative. When you put together the mass energy of the quarks, the kinetic energy, and the binding energy, you get the total energy of what we call a "bound system of $\text{uud}$ quarks." Why not just call it a proton? Well, there is actually a particle exactly like the proton but with a higher mass, the delta baryon $\Delta^+$. Technically, a $\text{uud}$ bound system could be either a proton or a delta baryon. But we've observed that when you put these three quarks together, you only ever get $\mathrm{p}^+$ (with a mass of $938\ \mathrm{MeV/c^2}$) or $\Delta^+$ (with a mass of $1232\ \mathrm{MeV/c^2}$). You can't get any old mass you want. This is a very strong indication that the mass of a $\text{uud}$ bound state is quantized in the second sense. Now, the calculations involved are very complicated, so I'm not sure if the operator which produces these two masses as eigenvalues can be derived in detail, but there's basically no doubt that it does exist. You can take other combinations of quarks, or even include leptons and other particles, and do the same thing with them - that is, given any particular combination of fundamental particles, you can make some number of composite particles a.k.a. bound states, and the masses of those particles will be quantized given what you're starting from . But in general, if you start without assuming the masses of the fundamental particles, we don't know that mass is quantized at all. 
126	physics	 One common way of making these measurements is gravitational lensing. Basically, astronomers look at some distant object which is located directly behind the galaxy in question. Since the galaxy is so massive, it bends the light from the more distant object around it, so we see an image of the object displaced by some angle from where it actually is in the sky. For a distant object in the right position, we can see multiple images, one from light deflected to the left and one from light deflected to the right. Measuring the angular separation between them allows you to compute the angle by which the light was bent, and in turn to determine the mass of the galaxy required to produce that deflection. For galaxies that are less massive but closer, close enough to resolve the spectra of individual stars, we can measure the Doppler shifts of the spectra of stars on the advancing side of the galaxy and on the receding side, and taking the difference gives (twice) the tangential speed of the stars at that radius, which is related to the amount of mass contained within that radius. So measuring the spectra of stars at the edge of the galaxy, or in e.g. a globular cluster that orbits the galaxy, can give you the total mass contained in the galaxy. 
127	physics	 First, I want to say upfront that this question need not dissolve into arguments and discussion. This question can and should have a correct answer, please don't respond with your opinions. GNUplot is very pervasive in Physics research. Many of the plots appearing in things like PRL and JPB are made in GNUplot. Why is this the case. There are much more modern tools for doing these types of graphs, and from my uninformed position, it appears that this would be easier. One obvious and relevant first point to make is that GNUplot is free and opensource. I respect this, but do not expect that this is the primary reason. I am hoping for answers that specify what GNUplot can do that can be achieved in other programs efficiently, say Mathematica. If there are none of these, and the reason is simply tradition/resistance to change, that is also fine, but I expect there must be some specific tasks one wishes to perform. Thanks! 
128	physics	 As many others said, the Sun feels the same force towards Earth as the Earth feels towards the sun. That is your equal and opposite force. In practice though the "visible" effects of a force can be deduced through Newton's first law, i.e. ${\bf F} = m{\bf a}$. In other words, you need to divide the force by the mass of the body to determine the net effect on the body itself. So: ${\bf F_s} = {\bf F_e}$&#xD; ${\bf F_s} = m_s {\bf a_s}$&#xD; ${\bf F_e} = m_e {\bf a_e}$&#xD; therefore, $m_s {\bf a_s} = m_e {\bf a_e}$&#xD; and ${\bf a_s} = {\bf a_s} \frac{m_e}{m_s}$&#xD; Now, the last term is $3 \cdot 10^{-6}$! This means that the force that the Earth enacts on the sun is basically doing nothing to the sun. Another way of seeing this: $F = \frac{G m_s m_e}{r^2}$&#xD; $a_s = \frac{F}{m_s} = \frac{G m_e}{r^2}$&#xD; $a_e = \frac{F}{m_e} = \frac{G m_s}{r^2}$&#xD; $\frac{a_s}{a_e} = \frac{m_e}{m_s} = 3 \cdot 10^{-6}$&#xD; Again, the same big difference in effect. Regarding the centripetal force, it is still the same force. Gravity provides a centripetal force which is what keeps Earth in orbit. Note It's worth pointing out that the mass that acts as the charge for gravity, known as gravitational mass is not, a priori, the same mass that appears in Newtons's law, known as inertial mass . On the other hand it is a fact of nature that they have the same value, and as such we may use a single symbol $m$, instead of two, $m_i$ and $m_g$. This is an underlying, unspoken assumption in the derivation above. This is known as the weak equivalence principle . 
129	physics	 What would be the implications to the Standard Model if the Higgs Boson hadn't been found with the LHC? Also, if the Higgs Boson had not been found with the LHC, would it have been successfully proven as non-existent? Or would we just wait for an experiment with higher energy ranges? 
130	physics	 Penrose's Conformal Cyclic Cosmology‎ is such an example. There is late time acceleration due to Lambda, but no inflationary period in the beginning. The late universe is equated to the beginning (of the next big bang cycle) through a conformal factor which maintains the physics. 
131	physics	 Wikipedia actually has a very nice graphic with this information (which roughly agrees with what I remember hearing from people "in the know"): The point is that there are both lower and upper bounds on the mass of the Higgs boson. The LHC should be able to cover pretty much the entire range that has not yet been searched, so if it doesn't find the Higgs, we can be fairly confident that something is wrong with the Standard Model. Now, the question is, what could be wrong? Well, there are various possibilities. At the simple end, it's possible that there is more than one Higgs boson. The simplest possible model has only one Higgs boson, and for obvious reasons that's the model that many people are hoping is correct, but it's perfectly possible that there could be a multiplet of several Higgs particles instead. If there is more than one, I'm not sure how exactly that would change the lower and upper bounds on the mass range, but I believe that there is some possibility that if there is a Higgs multiplet, all the particles could have higher masses than we would be able to detect. (I used to know more about this but it's been a little while) At the other extreme, it could be that the whole theoretical framework of the Standard Model is incorrect. That seems pretty unlikely, since pretty much every prediction the SM has made has turned out to be spot on (except for the presence of the Higgs, of course, but that's still an open question). There are definitely alternate theories waiting in the wings that will be receiving quite a bit more attention if the Higgs is not found. 
132	physics	 In order to calculate the average speed you have to weight the time of the different parts of the trip, and not with the distance covered in the same parts! So the basic formula you hate to use is : $v_{avg}=S_{tot}/T_{tot}$ If your trip is divided into two parts - $S_1$ covered at speed $V_1$ and $S_2$ covered at speed $V_2$ - what you can't do is : $V_{avg}=\frac{V1\times S1+V2\times S2}{S_1+S_2}$ (i.e) actually what you did with yours: $\frac{1}2(40\ mph+60\ mph) = 50\ mph$, since in your example $S_1=S_2$. Whereas what you can do is : $V_{avg}=\frac{V1\times T1+V2\times T2}{T1+T2}$ That, given your input, can be written as $\frac{S_1+S_2}{S_1/V_1+S_2/V_2}$, which is indeed equal to $\frac{S_1+S_2}{T_1+T_2}$ 
133	physics	 I agree with David and his disagreement. My experience and personal opinion is that GNUPlot is simpler and quicker for the real time analysis and then u already have the graph in gnuplot.. why would u bother to change it ;) 
134	physics	 Deuteron ( 2 H) is composed of a neutron (spin-1/2) and a proton (spin-1/2), with a total spin of 1, which is a boson. Therefore, it is possible for two deuterons to occupy the same quantum state. However, the protons and neutrons inside are fermions, so they proton/neutron shouldn't be able the share states due to the exclusion principle. Why it does not happen? 
135	physics	 The main problem here is this: Newton gives us formulas for a force, or a field, if you like. Einstein gives us more generic equations from which to derive gravitational formulas. In this context, one must first find a solution to Einstein's equations. This is represented by a formula. This formula is what might, or may not, be approximately equal to Newton's laws. This said, as answered elsewhere, there is one solution which is very similar to Newton's. It's a very important solution which describes the field in free space. You can find more about this formula -- in lingo it's a metric, here: The fact that they are approximations fundamentally arises from different factros: the fact that they are invariant laws under a number of transformations, but mostly special relativity concerns - in other words, no action at a distance - is a big one. 
136	physics	 Pulsars are a label we apply to neutron stars that have been observed to "pulse" radio and x-ray emissions. Although all pulsars are neutron stars, not all pulsars are the same. There are three distinct classes of pulsars are currently known: rotation-powered, where the loss of rotational energy of the star provides the power; accretion-powered pulsars, where the gravitational potential energy of accreted matter is the power source; and magnetars, where the decay of an extremely strong magnetic field provides the electromagnetic power. Recent observations with the Fermi Space Telescope has discovered a subclass of rotationally-powered pulsars that emit only gamma rays rather than in X-rays. Only 18 examples of this new class of pulsar are known. While each of these classes of pulsar and the physics underlying them are quite different, the behaviour as seen from Earth is quite similar. Since pulsars appear to pulse because they rotate, and it's impossible for the the initial stellar collapse which forms a neutron star not to add angular momentum on a core element during its gravitational collapse phase, it's a given that all neutron stars rotate. However, neutron star rotation does slow down over time. So non-rotating neutron stars are at least possible. Hence not all neutron stars will necessarily be pulsars, but most will. However practically, the definition of a pulsar is a "neutron star where we observe pulsations" rather than a distinct type of behaviour. So the answer is of necessity somewhat ambiguous. 
137	physics	 The exclusion principle is for identical fermions; but as proton and neutron are different they can occupy the same quantum state in the deuteron: that's the key point. That's why you can have a bound state proton-neutron (the deuteron) and no neutron-neutron. Look at my answer to this question for more details. The main problem is that to have a bound state, you need a positive contribution in the exchange integral, requiring the nucleus to have parallel spins. This is possible in the case of the deuteron. Edit: I think you also mean: "if two deuterons are in the same state, which is allowed because they are boson, then the proton/neutron composing these deuterons should also be in the same state". The problem is that the exclusion principle does not work like that. If you consider the state vector of the deuteron you cannot after "go down one level" if you consider the deuteron as a boson and consider the individual nucleus. That's because the stage vector of the deuteron is not the one of a neutron + the one of a proton; it is a linear combination of both. If you try to construct a global state composed of two neutrons and two protons then you will see that you cannot have a bound state where the spins are all parallel. You will have to construct a state where the spins of the protons are anti parallel and the same for the neutrons. At the end you will have a nice state vector the Helium, but that's a more complicated story. 
138	physics	 This one state for deuteron is not a single state for both protons and neutrons. 
139	physics	 Everyone knows it is close to $c$, but how close? What are the recent results? 
140	physics	 It is technically impossible to measure the speed of such a particle directly; and it all depend on "which" neutrino you are talking about. The speed is related to the momentum and the momentum to the energy. So you can have a neutrino of some MeV of total energy, another one of some GeV, etc. But in any cases, the answer will be "very very close" to c. In all cases neutrinos can be considered as ultra relativistic: their total energy is much higher than their rest energy: $E_t &gt;&gt; m_0 c^2$. In that case, this relation holds: $E = p c$ where $ p = \gamma m_0 v$. The problem is that $m_0$ is not know with a high precision, it is of the order of the eV. You can do the math inserting a plausible energy for a solar neutrino, or for a neutrino coming from a high energy collision process, taking any reasonable value for the rest mass. Edit: To fix the ideas, the rest mass can be something of the order of 0.1 eV and a typical solar neutrino can be of the order of 10 MeV. That's a $\gamma = 10^8$ ! 
141	physics	 I'm interested in the process that goes from, for example, huge data sets of temperature, pressure, precipitation etc readings to a model of the atmosphere. I'd like to know about how much relies on a priori knowledge of Navier-Stokes, and how much curve fitting is done. Some sort of general overview of the process (ideally a fairly accessible one) would be perfect. Edit I'm looking for a reference, specifically. I know more or less the science from talking to people who do this stuff, but I would like to have some summary article or book chapter overview I can refer to in written work. 
142	physics	 I would like to have a good understanding of what is happening when you add salt to boiling water. My understanding is that the boiling point will be higher, thus lengthening the process (obtaining boiling water), but at the same time, the dissolved salt reduce the polarization effect of the water molecules on the heat capacity, thus shortening the process. Is this competition between these two effects real ? Is it something else ? 
143	physics	 My favourite for pure classical mechanics is generally the book by Goldstein which includes the Lagrangian and Hamiltonian methods (although I'm not sure about symplectic geometrical and mathematical foundations). If you want a firm look at curved spacetime manifolds (for vectors, one forms, tangent bundles etc.) I would recommend Carroll's Spacetime and Geometry but it deals with the mathematical backing for General Relativity, which is classical mechanics, but on a curved spacetime. 
144	physics	 Much of how you answer this question comes down to your view of the wavefunction or state. If you think that the quantum state is a state of reality (that is, an ontic state), then you must either reproduce the predictions of orthodox (Copenhagen) QM without the measurement postulate or you must explain why nature provides two forms of evolution. The former view is basically the Many Worlds Interpretation, which I feel a great degree of attraction to, as it postulates only unitary evolution, and explains measurement as being an emergent, rather than fundamental, effect. On the other hand, if you hold that the wavefunction is a state of knowledge (epistemic) about some other underlying ontic state, then measurement collapse represents not a true evolution, but a discontinuous change in your knowledge about a system. Alternative formulations of quantum mechanics, such as Bohmian mechanics, explain this in a mathematically rigorous way, but that some find unsatisfying. Each of these approaches (and the many more I didn't mention) suggests where to look for the next physical theory, and so the question should eventually be experimentally decidable. For now, though, we must rely on mathematics, physical intuition and rational argument. 
145	physics	 I was wondering if anyone put together a law to describe the rising temperature of the water coming out of a tap. The setup is fairly simple: there's a water tank at temperature T, a metal tube of length L connected to it and a tap at the end where temperature is measured. The water flows at P l/s. Given that the metal tube is at room temperature initially, what law describes the temperature of the water at any instant? What is the limit temperature of the water? Thanks. 
146	physics	 The answer is going to depend on the heat transfer coefficient between the tube and the surrounding room (unless you specify that the tube is held at constant temperature), the heat transfer coefficient between the tube and water, the outside &amp; inside diameter of the tube, and the length of the tube. This is a moderately involved heat transfer problem, unless additional constraints are provided to simplify it. An excellent resource for understanding the mathematics behind this sort of heat transfer problem can be found over here . It's very similar to the solution posted above by @Cedric, but may be a little easier for some to follow. 
147	physics	 The answers in this question: What is spin as it relates to subatomic particles? do not address some particular questions regarding the concept of spin: How are some useful ways to imagine a particle without dimensions - like an electron - to spin? How are some useful ways to imagine a particle with spin 1/2 to make a 360° turn without returning to it's original position (the wave function transforms as: $\Psi \rightarrow -\Psi$). When spin is not a classical property of elementary particles, is it a purely relativistic property, a purely quantum-mechanical property or a mixture of both? 
148	physics	 I can't believe nobody's mentioned Arnol'd's book "Mathematical Methods for Classical Mechanics" - it covers everything you ask for in the first paragraph quite elegantly (though sometimes somewhat tersely). 
149	physics	 How should one imagine a particle without dimensions - like an electron - to spin? You don't. If you want to imagine, then you think classically and it is just a particle spinning... Thinking like that doesn't give you any other insight of what spin really is (an intrinsic angular momentum, behaving like an [orbital] angular momentum). How should one imagine a particle with spin 1/2 to make a 360° turn without returning to it's original position (the wave function transforms as: Ψ→−Ψ) Just imagine it ... no big deal. Again, classically this is not possible, but quantically it is. When spin is not a classical property of elementary particles, is it a purely relativistic property, a purely quantum-mechanical property or a mixture of both? The spin of elementary particle is a pure quantum mechanical effect. Edit: See @j.c. comment. Relativity also plays a role. Any other interpretation/calculation requires things like commutator, symmetry properties and group theory. The parallel between "real spinning" and "spin" (which is just a name) comes from the fact that the spin operator needed to account for properties of elementary particles behaves (= has the same definition, based on commutators) like orbital angular momentum operator. This again comes from symmetry properties of ... nature. The goal of quantum physics is to provide a way to calculate properties. If you want to calculate or go deeper in the problem, then you don't need this classical interpretation. 
151	physics	 We can consider the following model: a tube of constant temperature $T_e$ of lenght L, radius $r$ where water is flowing uniformly at a speed $v$ (that you can obtain from your flow $P$). A "slice" of water travels an interval $dx$ in a duration $dt = \frac{dx}{v}$. The tube will contribute to the "heating" of the water by $\frac{dQ}{dt} = (T-T_e) k 2 \pi r dx$ where $k$ is the conductivity and where we use a very simple model (in particular for the radius, we do not distinguish external and internal radii). During this interval the temperature $T(x)$ of the water will vary by $dT = -\frac{dQ}{c \rho dV}$ where $C$ is the heat capacity at constant pressure of water, and where $dV = 2 \pi r dx$. Replacing we have $\frac{dT}{T-T_e}=-\frac{k}{\rho C v} dx$ whose solution, if the temperature in the tank (ie x = 0) is $T_t$ : $T(x) = (T_t - T_e) e^{(-\alpha x)}+T_e$ where $\alpha = \frac{k}{\rho C v}$. Depending on the lenght of the tube you have the temperature at the tap. 
152	physics	 I think the dominant effect might actually be the fact that the salt you add might not be at boiling temperature. But this is just based on the fact that the boiling-point elevation due to salt in water is actually quite low for typical amounts of salt used in cooking, say. I'm not too familiar with the second effect you mention though. 
153	physics	 I would suggest learning about some of the formulas relating them - that way you're not just memorizing things but actually have some grasp of what goes into them. In particular, I only really know about stress and strain, and it's because I think of them as being the analogue in linear elasticity theory of "force" and "displacement" in Hooke's law. 
154	physics	 According to the wikipedia article , capacitative sensors detect things with conductive or dielectric properties; so to me, that means these sensors detect a contrast in the electrical properties between your finger, say, and the air around it. I just tested this out on my touchpad with a metal ruler and if I pushed down, I got a little bit of response. 
155	physics	 I think this may be a summary of other answers, but there are a couple things going on here. First, neutrons are electrically neutral, so an obvious force is the Van der Waals force. However, due to quantum mechanics and the Pauli exclusion principle (as noted above), neutrons' wave functions cannot (ish) overlap and so they are subject to the Neutron Degeneracy Pressure . I think if you understand these two concepts, then you should have a fairly good grasp on what is going on between neutrons in a neutron star. 
156	physics	 Although I've never seen it myself, I hear the northern lights are a sight to be seen! I know they're related to the Earth's magnetic field but I don't know much more about them. What is the physical phenomenon that creates the northern lights? 
157	physics	 Climate modelling is a giant science of its own, and the proportion of CFD/statistics depends on the particular model. In general, what models do is first a simple (often uncompressable) large-scale CFD to advect the scalar fields and then apply a bunch of subrutines simulating small scale and more complex processes, like radiation transfers, heat transfers, cloud dynamics, precipitation, evatranspiration and interactions with land/ocean. Those are mainly governed by some phenomenological theories, based either on some subscale simulations, approximated theories or purely statistical models. The proper construction of that step depends on the purpose of a model and has more to do with art than science (i.e. balancing accuracy and computational time). 
158	physics	 The ionized particles from mainly solar wind are caught and trapped by Earth magnetic field, which behaves like a magnetic bottle. (The region in which ions are trapped is called Van Allen radiation belts.) This trap is weaker in the polar regions, and there the ions are mainly released into the denser parts of atmosphere. There they collide with air particles (mostly N$_2$ and O$_2$) causing their fluorescence seen as the northern lights. 
159	physics	 In this question I asked about gravity and in the answers it came up that the magnitude is equal (of the gravity acting on the Sun and the of the gravity acting on the Earth) Does magnitude simply mean it's strength? 
160	physics	 I'm trying to explain elementary mechanics - without the benefits of calculus or even algebra - and struggling. I'd like to find reasonable ways to demonstrate Newton's laws, minimally, and possibly continuing though basic projectile motion, although that may be too much to hope for. 
161	physics	 Sure - a force has a magnitude (amount) and direction (vector). 
162	physics	 A force is a normal vector and a vector is characterized by its magnitude ( its norm, "lenght" in graphical representation ) and direction. 
163	physics	 In the case of centripetal forces, like gravity, force is always directed towards the center of mass - it's a radial force. We can therefore study most of its properties by calculating its strength. In particular, with gravity it is typical to use polar coordinates (e.g., in 2D, use angle and radius instead of $x$ and $y$). This leads to a gravitational force which only has a radial component. In this particular case the magnitude of the force vector is the same as the radial component, so it's really easy to calculate. In the general case, the magnitude is the length of the force vector calculated by the square root of the dot product of the vector by itself. In classical coordinates $(x,y)$ $|F| = \sqrt{(F_x^2 + F_y^2)}$ where $F_x$ and $F_y$ are the $x$ and $y$ components of the force vector ${\bf F}$. 
164	physics	 Just some quick ideas.. the question is fairly intriguing: i'm gonna think about it again. First law of Newton: i think that those dry ice containers that mimics the motion without friction can be found quite cheaply and can be very useful: If set in motion it will keep going If connected to a dynamometer from one side and an eletric mini4WD in the other side can demonstrate (with no math at all.. just common sense) that an object stay still not only if no forces are acting upon it but also if the sum of the force actin on an object is zero Second Law: using the same electic mini 4WD as before, measure witha dynamometer the force of the of the motor (u already did that in the previous experiment ;) ) and then vary the mass of the car. It will be quite evident (always thank to the common sense) that the heaviest car will accelerate less than the other.. Third law: take two object of similar but not equal mass and compress a spring between them: when you will release (yeah at the same time would be better:P ) them both of them will move in opposite direction.. that can be quite trickier to explain but the experimental evidence shuold be quite visible! 
165	physics	 Measuring the acceleration of gravity with a pendulum seems to be a good candidate. You don't have to explain the maths. It's the first physics experiment I did, but I was older. On the other hand, it could work with a good teacher. 
166	physics	 Energy production is one of the burning issues for humankind. There has been some talk about future energy technologies including Fusion, Anti-matter annihilation and Zero-point-energy (from most to least plausible). I'm interested in hearing what people know about developments in the field what they think will be the next real breakthrough in energy technologies. 
167	physics	 The source of Earth's magnetic field is a dynamo driven by convection current in the molten core. Using some basic physics principles (Maxwell's equations, fluid mechanics equations), properties of Earth (mass, radius, composition, temperature gradient, angular velocity), and properties of materials (conductivity and viscosity of molten iron) or other relevant facts, is it possible to estimate the strength of the field to order of magnitude (about one gauss)? Descriptions I've seen of the geodynamo all refer to extensive numerical computation on a computer, but can we get a rough idea with simple estimation? 
168	physics	 A process is reversible if and only if it's always at equilibrium during the process. Why? I have heard several specific example of this, such as adding weight gradually to a piston to compress the air inside reversibly, by why should it be true in general? EDIT: Here is something that would firmly convince me of this: Suppose I have a reversible process that is not always in equilibrium. Describe a mechanism for exploiting this process to create a perpetual motion machine. 
169	physics	 Experimental measurements of neutrino speed have been performed. But thus far, they have all found that the neutrinos were going so close to the speed of light that no difference was detectable within the precision of the experiments. A couple measurements are described here: 
170	physics	 The next serious advance that is not an speculative/fringe idea is most likely to be fusion power . Harnessing the power of nuclear fusion has long been a goal for energy production since the first hydrogen bomb was created in the 1950s. Creating controlled fusion, rather than the chaotic variety has proven a rather challenging task here on Earth however. (The sun does it quite easily largely thanks to its huge gravitational field.) Of particular note is the ITER project , currently ongoing in Southern France. It is an international collaborative project with the goal of creating a sustainable fusion reactor that produces net energy. (You can read information on the site for the specific target goals/criteria). This is essentially the last step in the 'proof of concept' stage of sustainable nuclear fusion for providing energy, and is to many looking quite promising. It is however only a scientific experiment still, and nuclear fusion power plants are a bit further off in the future, even given success of ITER. In fact, the proposed successor to ITER is DEMO , a reactor that aims to produce over an order of magnitude more power. Once testing is complete, the idea is to turn it into the world's first fusion power plant. With any luck, this may just be the breakthrough humanity has been waiting for. It will certainly be a revolutionary event whenever it occurs, and likely render fossil fuels totally obsolete. 
173	physics	 I would like to know if acceleration is an absolute quantity, and if so why? 
176	physics	 Because a reversible process must not increase the entropy in the system. Any change due to lack of equilibrium will lead to an increase in entropy, and thus irreversibility. You can visualize this by thinking of two bottles, one full of gas and one with vacuum. If the bottles were to be connected, atoms would randomly migrate between the two bottles leading to a system with the gas divided between the containers with equal pressure. Thus, the disorder of the system has increased, because the atoms that were all sorted in one bottle initially are now subdivided in two partitions of a much larger volume. In other words, the lack of equilibrium in pressure leads to an uniformization of the same, and to an irreversible increase in entropy. 
177	physics	 In standard Newtonian mechanics, acceleration is indeed considered to be an absolute quantity , in that it is not determined relative to any inertial frame of reference (constant velocity). This fact follows directly from the principal that forces are the same everywhere, independent of observer. Of course, if you're doing classical mechanics in an accelerating reference frame, then you introduce a fictitious force, and accelerations are not absolute with respect to an "inertial frame" or other accelerating reference frames - though this is less often considered, perhaps. Note also that the same statement applies to Einstein's Special Relativity. (I don't really understand enough General Relativity to comment, but I suspect it says no, and instead considers other more fundamental things, such as space-time geodesics.) 
178	physics	 Acceleration will be the same in any two frames that are moving with constant speed with respect to each other (and may also be rotated and translated). However, if you consider two frames that have relative rotation or acceleration, the acceleration of an object will be different in the two frames. 
179	physics	 Absolutely not. An observer in free fall and an observer in zero gravity both experience and observe no acceleration in their frame of relevance. One, however, is actually in an accelerating frame of reference. 
180	physics	 Let me give a try. When we construct a theory, we suspect that the objects it deals with can be rather complicated. It is natural that we want to find the simplest «building blocks» which the complicated objects are made of. If our theory were absolutely arbitrary, we won't be able to classify these simple building blocks at all. Fortunately, when constructing theories we note that the lagrangian we specify and the vacuum state have certain symmetries. Once we noted it, then it is pure math to show that the simple objects in our theory should be classified according to representations of the symmetry group of the lagrangian and the vacuum state. Note that there are some symmetries which are obvious to us, which we perceive (like invariance under the Poincare group), and there are some symmetries which we invent (like non-abelian gauge symmetries). In the latter case we know that, by construction, all the macroscopic states (including the vacuum state) must be invariant under this new internal symmetry group. This gives us a short-cut to the assertion that the simple object in our theory must be classified according to the representations of the new group. And what concerns the specific question: so the fundamental particle is acting on the quantum states? When we say that a particle or a field is in representation R of group G, we do not mean that the particles are associated with matrices of representation R acting on something else. We rather mean that the particle can be written in terms of eigenstates of matrices representing operators in R. So, it is the symmetry group transformations that act on the particles. 
181	physics	 I guess the simplest answer is just to carefully read you own words again. A reversible process is the one that can be made flow backwards. It is intuitive to think that it can be made flow backwards at any time we wish. But if the system were in a non-equilibrium state, one would need to wait a bit until it goes to equilibrium before trying to drive it back. So, it does not satisfy our desire to have the system under the control at any time. 
182	physics	 One answer might be Benford's law which state that the probability of lediding digit being 1 among natural constants is approx 30% How to interpret Benford's law: Any constant and measurment of "new" phenomen can be in any range. Ok max magnitude for known universe is approx 10^60, but still. Picking random number from this range is not evenly distributed but logarithmic distributed. example: We pick random number from 1 to 10^10 There is equal probability to pick number from 1 to 10^5 and from 10^5 to 10^10, this way we can prove that there is most likely to pick number that begins whit 1 and least likely whit 9. We can check this rule just by looking on logarithmic scale or doing some math. Measuring of earth field, gravity or something else obeys this law and this is the reason that is most likely that you will measure 1*10^n. Other reason that field is 1 gauss is that 1 is just magnitude/order of field. (by the way wikipedia article about this topic says that it is between 0.3 and 0.6 gauss) 
183	physics	 I've finally figured it out. First, let's define precisely what it means for some quantity to be absolute or relative. In the context in question, it has to do with whether a quantity is absolute (that is, has the same value) or relative (that is, has different values) when measured by two inertial observers moving with respect to one another. Of course, first we need to define what an inertial observer is: it's an observer for which Newton's laws are applicable without having to resort to adding fictitious forces. Ok, so now we have two observers, Alice and Bob, both of which are inertial. They both observe the motion of some object. Let the index 1 correspond to quantities measured in A's reference frame and the index 2 correspond to quantities measured in B's reference frame. The position of the object is clearly a relative concept, since r₂ = r₁ + u t (where u is the velocity of Bob with respect to Alice, and is constant since they're both inertial observers). Note that the time, t, is the same for both observers, as it must be according to Newtonian Mechanics. The object position is a relative concept because r₂ ≠ r₁. Now, take the time-derivative of both sides and we get v₂ = v₁ + u that is, the velocity of the object with respect to one observer is different than the velocity of the same object with respect to the other observer. Hence, velocity is a relative quantity in Newtonian Mechanics. Next, take the time-derivative of both sides once again, and we obtain a₂ = a₁ (since u is constant). Thus, the acceleration of the object is the same in both reference frames. Acceleration, therefore, is absolute in Newtonian Mechanics. When we take into account the theory of relativity, then time flows at different rates for different inertial observers and the result above for the acceleration is no longer true. 
184	physics	 When you're driving, make them notice that if you're going at a constant speed, no matter how fast, they don't "feel" it. Yet, when accelerating from 0-40 they'll be pushed in their seats. 
185	physics	 As I understand the Higg's boson can be discovered by the LHC because the collisions are done at an energy that is high enough to produce it and because the luminosity will be high enough also. But what is needed to claim a real "discovery" ? I guess there is not one event saying "hey, that's an Higg's boson" ... I also guess that this was the same kind of situation when the top quark was discovered. How does it work ? Edit: There is a nice introduction to the subject on this page of the CMS experiment , and the various ways to detect it, for example through the following process. 
186	physics	 We have to analyze this video Givens: An applied net torque due to the wind on the windmill is equal to 1500 N*m. Each (of the 3) propeller props weighs approximately 45 Kg and has a Moment of Inertial equal to about 65% of that of a rod of the same mass being spun about its end. This torque is applied for approximately 45 seconds before the explosion, prior to which the windmill was at rest. Question: What was the angular acceleration caused by the torque? So here's my attempt at it: T=Ia (a is alpha) T=ML^2/3 * a * .65 (due to the whole 65% thing. Actually not sure if I should put 3*M for each propeller) And so this is where I get stuck. I'm not given L, so I'm not sure how to work around this. I could also use $T=1/2MR^2\times a$, but then I dont know R. 
187	physics	 I am always fond of the experiment of "shooting" a rubber ball and dropping the same ball, and seeing that both of them will hit the ground at the same time, showing that the vertical and horizontal components of velocity are independent. 
188	physics	 I'm not a particle physicist, but here's a little overview from what I understand. Colliding particles, in particular hadrons (being composed of three quarks and a field of gluons), are quite capable of generating very "messy" collisions, the more so at higher energies. By "messy" I mean that the variations on the possible results of the collision (the number of different Feynmann diagrams) is pretty large. Of course, certain outcomes have much higher outcomes than others, and the probabilities of such can be estimated in quantum field theory. In any case, there can be decays into all sorts of fundamental particles (with different charges, spins, masses, etc.), and then further decays, and so forth. Verifying that a particle (here the Higgs boson) is really what the theory predicts takes lots of runs of the experiment, and is largely just a complex probability game. Given that the properties of the Higgs aren't exactly known, what particle physicists are looking for is signs of a missing particle . i.e. A violation of some conservation law in the collision (typically energy or momentum). This is a good hint that there's an unknown particle that hasn't been accounted for. (For example, neutrinos are far too weakly interacting to be detected directly, and were first discovered by noticing some small energy was missing). At the moment, all we really know from previous experimental data is that the lower bound for the Higgs boson mass is 115 GeV/c² - pretty high, but theoretically well within the range of the LHC. This helps us a bit in knowing where to look, but at the end of the day, it's smashing together protons over and over again that rather crudely gives you the final discovery! 
189	physics	 The first thing I would point out to you is that $\tau = \frac{1}{2} MR^2 \alpha$ is really just $\tau = I\alpha$, with a particular choice of $I$. Is that choice appropriate for this problem? (Ask yourself the same thing any other time you consider using $\tau = \frac{1}{2} MR^2\alpha$.) Next, note that the moments of inertia of different parts of the windmill do add up to produce the total, just like with mass. You can't just use the moment of inertia of one propeller prop, you have to calculate the total moment of inertia. Finally, consider this: what information can you get from the video, that could supplement the 3 "givens"? There's no length scale in the video, so you can't measure the length of a prop directly, but there is a time scale. What can you do with that? 
190	physics	 Interactions merely involve a correlation developing. For example, if an electron is put through a Stern-Gerlach apparatus, a correlation develops between the distance travelled in the x direction and the distance deviated in the y direction. It is reversible. The measurement which occurs when the particle hits the photographic plate is irreversible. It is associated with irreversible dissipation, i.e. entropy generation. This approximation can itself be dissected further, but it gets very tricky. A really good (1983) book is by Wheeler and Zurek, "The quantum theory of measurement" available as a djvu file at eds _Quan.html (8 MB, wait 30 sec for the download). [Now if I can only figure out how to work a reader for a Mac ...] 
191	physics	 Why does a magnetic core saturate? What is its actual mechanism? 
192	physics	 From Wikipedia: - Ferromagnetic materials like iron that show saturation are composed of microscopic regions called magnetic domains that act like tiny permanent magnets. Before an external magnetic field is applied to the material, the domains are oriented in random directions. Their tiny magnetic fields point in random directions and cancel each other out, so the material has no overall net magnetic field. When an external magnetizing field H is applied to the material, it penetrates the material and aligns the domains, causing their tiny magnetic fields to turn and align parallel to the external field, adding together to create a large magnetic field which extends out from the material. This is called magnetization. The stronger the external magnetic field, the more the domains align. Saturation occurs when practically all the domains are lined up, so further increases in applied field can't cause further alignment of the domains. This is a simplified account; a more complete explanation can be found in Ferromagnetism. 
193	physics	 What are the best textbooks to read for the mathematical background you need for modern physics, such as, string theory? Some subjects off the top of my head that probably need covering: Differential geometry, Manifolds, etc. Lie groups, Lie algebras and their representation theory. Algebraic topology. 
194	physics	 An important point about relativity is that it is not quite like the physics you learn in an introductory physics class. There, you learn about Newton's Laws or Snell's Law or Lenz's Law, etc. Those are all laws that tell things how to act; they tell mass how to respond to force, or light how to bend, or currents which way to run. Relativity is different in that it provides a set of meta-laws, or laws that the other laws of physics must obey. It doesn't directly tell things what to do. The classic example is Maxwell's Equations. These are laws that tell charged particles and electromagnetic fields how to act. It turns out that these laws obey a certain mathematical criterion, called "Lorentz Invariance", that is required by relativity. So Maxwell's equations are good relativistic laws. They obey the meta-laws. On the other hand, Newton's laws (of motion) are not good relativistic laws. They don't obey the meta-laws. So in relativity, we need a slightly new set of laws to describe how mass responds to force. As for what the meta-laws are, they were outlined by Nick Gotch as "basic ideas" above. Those basic ideas turn out to be equivalent to Lorentz invariance. 
195	physics	 NOTE: I recommend reading Noldorin's answer first, for useful background information, and Matt's answer afterward if you want additional detail Noldorin is right that there isn't a single event that you can look at and identify a Higgs boson. In fact, unless the theories are drastically wrong, the Higgs particle is unstable and it has an exceedingly short lifetime - so short that it won't even make it out of the empty space inside the detector! Even at the speed of light, it can only travel a microscopic distance before it decays into other particles. (If I can find some numeric predictions I'll edit that information in.) So we won't be able to detect a Higgs boson directly . What scientists will be looking for are particular patterns of known particles that are signatures of Higgs decay. For example, the standard model predicts that a Higgs boson could decay into two Z bosons, which in turn decay into a muon and antimuon each. So if physicists see that a particular collision produces two muons and two antimuons, among other particles, there's a chance that somewhere in the mess of particles produced in that collision, there was a Higgs boson. This is just one example, of course; there are many other sets of particles that the Higgs could decay into, and the large detectors at the LHC are designed to look for all of them. Of course, Higgs decay is not the only thing that could produce two muon-antimuon pairs, and the same is true for other possible decay products. So just seeing the expected decay products is not a sure sign of a Higgs detection. The real evidence is going to come from the results of many collisions (billions or trillions), accumulated over time. For each possible set of decay products, you can plot the fraction of collisions in which those decay products are produced (or rather, the scattering cross section, a related quantity) against the total energy of the particles coming into the collision. If the Higgs is real, you'll see a spike, called a resonance , in the graph at the energy corresponding to the mass of the Higgs particle. It'll look something like this plot, which was produced for the Z boson (which has a mass of only 91 GeV): The image is from , which is actually a pretty good read. Anyway, to sum up: the main signature of the Higgs boson, like other unstable particles, will be this resonance peak that appears in a graph produced by aggregating data from many billions or trillions of collisions. Hopefully this makes it a bit clearer why there's going to be a lot of detailed analysis involved before we get any clear detection or non-detection of the Higgs particle. 
196	physics	 I've always heard people saying, "Faster than light information transfer can't happen because it would violate causality! Effects can precede their causes!" However, I'm trying to think of a situation where this would happen. I'm sure it has something to do with relativity or something like that. I sort of understand that people going faster perceive time slower. Can someone help illuminate this for me by showing a scenario where causality is clearly violated due to FTL information transfer? 
197	physics	 I hope this is the right word to use. To me, these forces seem kind of fanciful (except for General Relativity and Gravity, which have a geometric interpretation). For example, how do two charged particles know that they are to move apart from each other? Do they communicate with each other somehow through some means? I've heard some people tell me that they bounce together messenger photons. So does one electron receive a messenger photon, go, "Oh hey, I should move in the direction opposite of where this came from, due to the data in it", and then move? Aren't photons also associated with energy, as well? Does this type of mediation imply that electrons give off energy in order to exert force on other electrons? Every electron is repelled by every other electron in the universe, right? How does it know where to send its force mediators? Does it just know what direction to point it in? Does it simply send it in all directions in a continuum? Does that mean it's always giving off photons/energy? I'm just not sure how to view "how" it is that electrons know they are to move away from each other. These questions have always bugged me when studying forces. I'm sure the Standard Model has something to shed some light on it. 
198	physics	 I have a particle system of seven protons and seven (or sometimes eight) neutrons (each formed by their appropriate quarks, etc.) bound together in a state that can be macroscopically described as a nucleus. If relevant, there are also about seven electrons that are bound to this arrangement. These particle systems are usually found in pairs, bound to eachother. Macroscopically, this can be modeled as the elemental Nitrogen ($N_2$), and in other disciplines (such as chemistry), it is treated as a basic unit. We know that at a certain level of thermal energy, this system of elementary particles exist inert and packed together in what can be macroscopically described as a "liquid". We know that this is this temperature is about 77.36 Kelvin (measured experimentally) at the most. Any higher and they start repelling each other and behave as a macroscopic gas. Is there any way, from simply analyzing the particles that make up this system (the quarks making up fourteen protons and 14-16 neutrons, the electrons) and their interactions due to the current model of particles (is this The Standard Model?), to find this temperature 77.36 Kelvin? Can we "derive" 77.36 K from only knowing the particles and their interactions with each other, in the strong nuclear force and electromagnetic force and weak nuclear force? If so, what is this derivation? 
199	physics	 Suppose you and I have a conversation from a long distance away. We're at rest with respect to each other and communicate much faster than light. I say "How are you", and you wait a short time and say, "I'm fine thanks." From our point of view, you were responding to my question. However, from a reference frame moving from me to you at relativistic speed, your clock is significantly ahead of mine (a relativistic effect). This means that although you thought you received the message shortly after I sent it, in this frame you didn't. You actually received the message at an earlier time (before I sent it), but you thought it was later because your clock is ahead. From your and my point of view, the order of events is I say "How are you?" You hear me say "How are you?" You pause a short time. You say, "I'm fine thanks." I hear you say, "I'm fine thanks." From the frame moving from me to you, the order of events is You hear me say "How are you?" You pause a short time. You say "I'm fine thanks." I say, "How are you?" I hear you say, "I'm fine thanks." The fact that the order of events changes between reference frames is simply part of relativity, with or without faster-than-light communication. However, it seems strange in this scenario because you are responding to me. Presumably, if I had said, "Where are my car keys?", you would have chosen a different response than "I'm fine thanks." How then is it possible that you responded to my greeting before I uttered it, at least in some frame? I'm not sure if this "violates causality", but it's unintuitive. 
200	physics	 Brief answer: Read only the bold part (and ignore grammar then). The answer you already mentioned lies in Quantum Field Theory (QFT). But to fully understand it, you must give up a particle as a point-like thing that is well-localized . There is one Quantum Field per sort of particle , e.g. the electron field for all electrons, and the photon field for all photons. (The fact that there is a single field for all electrons also results in the Pauli exclusion principle .) What you consider a particle is basically just a local peak in the respective particle field , but one cannot even say "This peak corresponds to electron A, this one to B". Now QFT, more specifically Quantum Electrodynamcis ( QED ), describes the local interaction between the electron field and the photon field . But since the fields have a dynamic, a local change induced in the photon field by the electron field will propagate with the speed of light (flat space assumed) and interact with the electron field in another place, thus creating the impression "Electron A emitted a photon that told electron B to interact electromagnetically". It's similar for the other interactions, there's a gluon field for the strong interaction ( Quantum Chromodynamics ), and for the electroweak interaction there's kind of a combination of the photon field and the weak-interaction-bosons. 
201	physics	 For example, how do two charged particles know that they are to move apart from each other? Do they communicate with each other somehow through some means? Yes, specifically the electromagnetic field. To give a simplistic view, a charged particle produces an electric field to indicate its presence and a magnetic field to indicate its motion. Any disturbance in these fields propagates outward at the speed of light. Another charged particle some distance away can react to the fields; if it "feels" the presence of an electric field, it will move in response. I've heard some people tell me that they bounce together messenger photons. So does one electron receive a messenger photon, go, "Oh hey, I should move in the direction opposite of where this came from, due to the data in it", and then move? The idea of the messenger photon is really just an analogy. Personally I don't think it's a very good one, but we don't really have anything better. The thing is, even though disturbances in the EM field propagate like waves, when a particle reacts to such a disturbance, it acts as though another particle collided with it. In order to make this fit with our intuition, we're forced to invent the idea that there are particles, or quanta , associated with the electromagnetic field, and we call them photons. Aren't photons also associated with energy, as well? Does this type of mediation imply that electrons give off energy in order to exert force on other electrons? Yep. To make sense of this, you really have to consider the whole system of both electrons, as well as the EM field itself. Each electron "feels" the electromagnetic field produced by the other, so both electrons' motions will change in such a way that the total energy is conserved. Every electron is repelled by every other electron in the universe, right? How does it know where to send its force mediators? Does it just know what direction to point it in? Does it simply send it in all directions in a continuum? Does that mean it's always giving off photons/energy? This is one of those cases in which it makes more sense to think of disturbances in the EM field as waves, which can certainly be radiated in all directions. However, the electron only disturbs the field when its motion (or quantum state, rather) changes . An electron stuck in a single quantum state, such as an atomic orbital, won't be radiating any energy. The bottom line is that I would advise you not to take the idea of "messenger photons" too literally. It's just a model that works in some situations but not in others. 
202	physics	 I just came from a class on Fourier Transformations as applied to signal processing and sound. It all seems pretty abstract to me, so I was wondering if there were any physical systems that would behave like a Fourier transformation. That is, if given a wave, a purely physical process that would "return" the Fourier transform in some meaningful way. Like, you gave it a sound wave and you would see, "Oh, there are a lot of components of frequency 1kHz...a few of frequency 10kHz...some of 500Hz..." I've seen things happening where, if you put sand on a speaker, the sand would start to form patterns on the speakers that are related to the dominant wavelengths/fundamental frequencies of the sound. Is this some sort of natural, physical fourier transform? 
203	physics	 Your ear is an effective Fourier transformer. An ear contains many small hair cells. The hair cells differ in length, tension, and thickness, and therefore respond to different frequencies. Different hair cells are mechanically linked to ion channels in different neurons, so different neurons in the brain get activated depending on the Fourier transform of the sound you're hearing. A piano is a Fourier analyzer for a similar reason. A prism or diffraction grating would be a Fourier analyzer for light. It spreads out light of different frequencies, allowing us to analyze how much of each frequency is present in a given source. 
204	physics	 Theoretically, yes it should be possible to derive the boiling point of diatomic nitrogen from fundamental forces. In fact, you don't even need to involve the strong force or weak force (or the strong nuclear force, which is sort of different). The strong forces bind the quarks together into nucleons and the nucleons together into nuclei, but they have essentially no effect on distance scales much larger than that of an atomic nucleus. So, for purposes of calculating the boiling point of nitrogen, you can treat the nucleus as basically a point charge. The only force that is relevant to calculating a boiling point is the electromagnetic force. Now the bad news: even something as simple as calculating the energy levels of helium, with 2 electrons, is impossible to do analytically. To analyze the behavior of even just those two electrons (and He nucleus) in detail, you need to use either perturbation theory or a numerical simulation, or both. And of course, the complexity increases with the number of particles, so simulating the 14 electrons and 2 nuclei of a nitrogen molecule is absurdly complicated. Perhaps it's been done, but I'm not a condensed matter physicist so I wouldn't know where to look for a reference. Maybe someone else can provide you with that information. If you were to calculate the boiling point of nitrogen, I believe the main effect that you'd take into account would be the instantaneous dipole interaction . According to the Wikipedia article, it gives an interaction energy in terms of the polarizabilities and ionization energies of the molecules. Those are the quantities that you would have to extract from your simulation and/or perturbative calculation of the dynamics of the nitrogen molecule, if you wanted to calculate the effect from first principles. $$E_{AB}^{\rm disp} \approx -{3 \alpha^A \alpha^B I_A I_B\over 4(I_A + I_B)} R^{-6}$$ (that formula is actually for monatomic noble gases, it may not apply to diatomic molecules) Once you get the interaction energy as a function of intermolecular separation $R$, you would then have to do either another numerical simulation, or a rather complicated calculation, to show that a large pool of nitrogen molecules subject to the given intermolecular force undergoes a phase transition at 77.36K (at standard pressure, I assume). There are various thermodynamic models you could use, some more accurate than others, but of course the more accuracy you want, the more computation power you'll need. I suspect that in order to get within a few degrees of the actual temperature, you would need to do something more computationally intensive than would be possible by hand. 
205	physics	 In imaging microscopy the 2D-Fourier Transform is a useful tool to analyse the images and improve the S/N ratio. I had your same opinion at the beginning but then my boss, who actually is a FTT entusiast, was able to convince me to the actual usefulness (comic) of the technique! 
206	physics	 Galileo's Leaning Tower of Pisa experiment because . 
208	physics	 M = mass of the sun m = mass of the earth r = distance between the earth and the sun The sun is converting mass into energy by nuclear fusion. $F = \frac{GMm}{r^2} = \frac{mv^2}{r} \rightarrow r = \frac{GM}{v^2}$ $\Delta E = \Delta M c^2 = (M_{t} - M_{t+\Delta t}) c^2 \rightarrow \Delta M = \Delta E / c^2$ $\rightarrow \frac{\Delta r}{\Delta t} = \frac{G}{v^2 c^2}.\frac{\Delta E}{\Delta t}$ Sun radiates $3.9 × 10^{26} W = \Delta E/\Delta t$ Velocity of the earth $v = 29.8k m/s$ There is nothing that is stopping the earth from moving with the same velocity so for centripetal force to balance gravitational force $r$ must change. Is $r$ increasing? ($\Delta r/ \Delta t = 3.26070717 × 10^{-10} m/s $) 
210	physics	 I think your reasoning is correct, but the values involved are very small. In one year r will increase by 1mm, so in 1 billion years it will have increased by 1000 km or by about0.01% 
211	physics	 That is basically correct, however that change is not very significant. The orbit of the planets in the solar system is chaotic over long periods of time (2 - 230 million years according to this wikipedia entry ), and this effect is relatively minor. Other causes for change of orbit include gravitational pull from other planets, collisions with asteroids, solar wind and other variables. 
212	physics	 Such a small magnitude makes this process negliable among other factors; indeed this result literally means that you don't need to care about this process until you build extreamly accurate theory of the full solar system dynamics. To the extent that is probably unreachable due to deterministic chaos. 
213	physics	 I think the reasoning has an error. It assumes $v$ is constant, but instead we ought to assume the angular momentum is constant. By dimensional analysis that leads to $r \propto \frac{L^2}{GM}$ so as $M$ decreases, $r$ increases (the original post had $r \propto M$, not $r \propto 1/M$. On the other hand, assuming a circular orbit seems dubious. As the other commenters said, this effect is minute. A significant effect on the orbit of the moon around the earth is tidal evolution, which does actually push the moon further away. See 
214	physics	 Imagine you're trapped inside a free falling elevator. Would you decrease your impact impulse by jumping during the fall? When? 
215	physics	 If you jumped just before impact, your speed towards the bottom of the elevator shaft would go down a little bit. But consider that the elevator falls tens of meters, while you jump about one meter. Your jumping ability is quite small and probably won't make a noticeable difference 
216	physics	 No. You'd reduce your impact velocity, but only with a negligible small fraction. 
110	stats	 It depends on where you apply the window function. If you do it in the time domain, it's because you only want to analyze the periodic behavior of the function in a short duration. You do this when you don't believe that your data is from a stationary process. If you do it in the frequency domain, then you do it to isolate a specific set of frequencies for further analysis; you do this when you believe that (for instance) high-frequency components are spurious. The first three chapters of "A Wavelet Tour of Signal Processing" by Stephane Mallat have an excellent introduction to signal processing in general, and chapter 4 goes into a very good discussion of windowing and time-frequency representations in both continuous and discrete time, along with a few worked-out examples. 
111	stats	 In R, the default setting for random number generation are: For U(0,1), use the Mersenne-Twister algorithm For Guassian numbers use the numerical inversion of the standard normal distribution function. You can easily check this, viz. It is possible to change the default generator to other PRNGs, such as Super-Duper,Wichmann-Hill, Marsaglia-Multicarry or even a user-supplied PRNG. See the ?RNGkind for further details. I have never needed to change the default PRNG. The C GSL library also uses the Mersenne-Twister by default. 
112	stats	 I see all my favorite blogs have been listed. So I'll give you this one: I Love Charts It's a bit light hearted. 
113	stats	 I have been looking into theoretical frameworks for method selection (note: not model selection) and have found very little systematic, mathematically-motivated work. By 'method selection', I mean a framework for distinguishing the appropriate (or better, optimal) method with respect to a problem, or problem type. What I have found is substantial, if piecemeal, work on particular methods and their tuning (i.e. prior selection in Bayesian methods), and method selection via bias selection (e.g. Inductive Policy: The Pragmatics of Bias Selection ). I may be unrealistic at this early stage of machine learning's development, but I was hoping to find something like what measurement theory does in prescribing admissible transformations and tests by scale type, only writ large in the arena of learning problems. Any suggestions? 
114	stats	 What statistical research blogs would you recommend, and why? 
115	stats	 We Love Datavis , a data visualization tumblog. 
116	stats	 Cosma Shalizi's blog , often talks about statistics, and is always interesting. 
117	stats	 is an aggregated blog from lots of blogs that talk about statistics using R, and the #rstats hashtag on twitter is also helpful. I write quite a bit about statistics and R in genetics research . 
118	stats	 In the definition of standard deviation, why do we have to square the difference from the mean to get the mean (E) and take the square root back at the end? Can't we just simply take the absolute value of the difference instead and get the expected value (mean) of those, and wouldn't that also show the variation of the data? The number is going to be different from square method (the absolute-value method will be smaller), but it should still show the spread of data. Anybody know why we take this square approach as a standard? The definition of standard deviation: $\sigma = \sqrt{E\left[\left(X - \mu\right)^2\right]}.$&#xD; Can't we just take the absolute value instead and still be a good measurement? $\sigma = E\left[|X - \mu|\right]$&#xD; 
119	stats	 There are many reasons; probably the main is that it works well as parameter of normal distribution. 
120	stats	 One way you can think of this is that standard deviation is similar to a "distance from the mean". Compare this to distances in euclidean space - this gives you the true distance, where what you suggested (which, btw, is the absolute deviation ) is more like a manhattan distance calculation. 
121	stats	 The squared difference has nicer mathematical properties; it's continuously differentiable (nice when you want to minimize it), it's a sufficient statistic for the Gaussian distribution, and it's (a version of) the L2 norm which comes in handy for proving convergence and so on. The mean absolute deviation (the absolute value notation you suggest) is also used as a measure of dispersion, but it's not as "well-behaved" as the squared error. 
123	stats	 Squaring the difference from the mean has a couple of reasons. Variance is defined as the 2nd moment of the deviation (the R.V here is (x-$\mu$) ) and thus the square as moments are simply the expectations of higher powers of the random variable. Having a square as opposed to the absolute value function gives a nice continuous and differentiable function (absolute value is not differentiable at 0) - which makes it the natural choice, especially in the context of estimation and regression analysis. The squared formulation also naturally falls out of parameters of the Normal Distribution. 
124	stats	 I'm a programmer without statistical background, and I'm currently looking at different classification methods for a large number of different documents that I want to classify into pre-defined categories. I've been reading about kNN, SVM and NN. However, I have some trouble getting started. What resources do you recommend? I do know single variable and multi variable calculus quite well, so my math should be strong enough. I also own Bishop's book on Neural Networks, but it has proven to be a bit dense as an introduction. 
125	stats	 Which is the best introductory textbook for Bayesian statistics? One book per answer, please. 
126	stats	 My favorite is "Bayesian Data Analysis" by Gelman, et al. 
127	stats	 Another vote for Gelman et al., but a close second for me -- being of the learn-by-doing persuasion -- is Bayesian Computation with R . 
128	stats	 In Plain English, how does one interpret a Bland-Altman plot? What are the advantages of using a Bland-Altman plot over other methods of comparing two different measurement methods? 
129	stats	 I quite like Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference by Gamerman and Lopes. 
130	stats	 I had a plan of learning R in the near future. Reading another question I found out about Clojure. Now I don't know what to do. I think a big advantage of R for me is that some people in Economics use it, including one of my supervisors (though the other said: stay away from R!). One advantage of Clojure is that it is Lisp-based, and as I have started learning Emacs and I am keen on writing my own customisations, it would be helpful (yeah, I know Clojure and Elisp are different dialects of Lisp, but they are both Lisp and thus similar I would imagine). I can't ask which one is better, because I know this is very personal, but could someone give me the advantages (or advantages) of Clojure x R, especially in practical terms? For example, which one should be easier to learn, which one is more flexible or more powerful, which one has more libraries, more support, more users, etc? My intended use : The bulk of my estimation should be done using Matlab, so I am not looking for anything too deep in terms of statistical analysis, but rather a software to substitute Excel for the initial data manipulation and visualisation, summary statistics and charting, but also some basic statistical analysis or the initial attempts at my estimation. 
131	stats	 Let me start by saying that I love both languages: you can't go wrong with either, and they are certainly better than something like C++ or Java for doing data analysis. For basic data analysis I would suggest R (especially with plyr). IMO, R is a little easier to learn than Clojure, although this isn't completely obvious since Clojure is based on Lisp and there are numerous fantastic Lisp resources available (such as SICP ). There are less keywords in Clojure, but the libraries are much more difficult to install and work with. Also, keep in mind that R (or S) is largely derived from Scheme, so you would benefit from Lisp knowledge when using it. In general: The main advantage of R is the community on CRAN (over 2461 packages and counting). Nothing will compare with this in the near future, not even a commercial application like matlab. Clojure has the big advantage of running on the JVM which means that it can use any Java based library immediately. I would add that I gave a talk relating Clojure/Incanter to R a while ago, so you may find it of interest. In my experience around creating this, Clojure was generally slower than R for simple operations. 
132	stats	 Coming from non-statistical background I found Introduction to Applied Bayesian Statistics and Estimation for Social Scientists quite informative and easy to follow. 
133	stats	 I don't know how to use SAS/R/Orange, but it sounds like the kind of test you need is a chi-square test . 
134	stats	 On smaller window sizes, sorting might work. Are there any better algorithms to achieve this? 
135	stats	 I believe that this calls for a two-sample Kolmogorov–Smirnov test , or the like. The two-sample Kolmogorov–Smirnov test is based on comparing differences in the empirical distribution functions (ECDF) of two samples, meaning it is sensitive to both location and shape of the the two samples. It also generalizes out to a multivariate form. This test is found in various forms in different packages in R, so if you are basically proficient, all you have to do is install one of them (e.g. fBasics ), and run it on your sample data. 
137	stats	 I recommend these books - they are highly rated on Amazon too: "Text Mining" by Weiss "Text Mining Application Programming", by Konchady For software, I recommend RapidMiner (with the text plugin), free and open-source. This is my "text mining process": Then you can start the work of classifying them. kNN, SVM, or Naive Bayes as appropriate. You can see my series of text mining videos here 
138	stats	 I'm interested in learning R on the cheap. What's the best free resource/book/tutorial for learning R? 
139	stats	 If I had to choose one thing, make sure that you read "The R Inferno" . There are many good resources on the R homepage , but in particular, read "An Introduction to R" and "The R Language Definition" . 
140	stats	 The official guides are pretty nice; check out . There is also a lot of contributed documentation there. 
141	stats	 Light-hearted: Indexed Also, see older visualizations from the same creator at the original Indexed Blog . 
142	stats	 After you learn the basics, I find the following sites very useful: R-bloggers . Subscribing to the Stack overflow R tag . 
143	stats	 Neural network may be to slow for a large number of documents (also this is now pretty much obsolete). And you may also check Random Forest among classifiers; it is quite fast, scales nice and does not need complex tuning. 
144	stats	 Quick-R can be a good place to start. A little bit data mining oriented R and Data Mining resources: Examples and Case Studies and R Reference Card for Data Mining . 
145	stats	 Possible Duplicate: Locating freely available data samples Where can I find freely accessible data sources? I'm thinking of sites like ? 
146	stats	 A while ago a user on R-help mailing list asked about the soundness of using PCA scores in a regression. The user is trying to use some PC scores to explain variation in another PC (see full discussion here ). The answer was that no, this is not sound because PCs are orthogonal to each other. Can someone explain in a bit more detail why this is so? 
147	stats	 Amazon has free Public Data sets for use with EC2. Here's a list: 
148	stats	 - is a good resource for free data sets. 
149	stats	 A principal component is a weighted linear combination of all your factors (X's). example: PC1 = 0.1X1 + 0.3X2 There will be one component for each factor (though in general a small number are selected). The components are created such that they have zero correlation (are orthogonal), by design. Therefore, component PC1 should not explain any variation in component PC2. You may want to do regression on your Y variable and the PCA representation of your X's, as they will not have multi-collinearity. However, this could be hard to interpret. If you have more X's than observations, which breaks OLS, you can regress on your components, and simply select a smaller number of the highest variation components. Principal Component Analysis by Jollife a very in-depth and highly cited book on the subject This is also good: 
150	stats	 For complete beginners, try William Briggs Breaking the Law of Averages: Real-Life Probability and Statistics in Plain English 
151	stats	 If the goal of the standard deviation is to summarise the spread of a symmetrical data set (i.e. in general how far each datum is from the mean), then we need a good method of defining how to measure that spread. The benefits of squaring include: Squaring always gives a positive value, so the sum will not be zero. Squaring emphasizes larger differences - a feature that turns out to be both good and bad (think of the effect outliers have). Squaring however does have a problem as a measure of spread and that is that the units are all squared, where as we'd might prefer the spread to be in the same units as the original data (think of squared pounds or squared dollars or squared apples). Hence the square root allows us to return to the original units. I suppose you could say that absolute difference assigns equal weight to the spread of data where as squaring emphasises the extremes. Technically though, as others have pointed out, squaring makes the algebra much easier to work with and offers properties that the absolute method does not (for example, the variance is equal to the expected value of the square of the distribution minus the square of the mean of the distribution) It's important to note however that there's no reason you couldn't take the absolute difference if that is your preference on how you wish to view 'spread' (sort of how some people see 5% as some magical thresh hold for p-values, when in fact it's situation dependent). Indeed, there are in fact several competing methods for measuring spread. My view is to use the squared values because I like to think of how it relates to the Pythagorean Theorem of Statistics: $c = \sqrt{a^2 + b^2}$ ...this also helps me remember that when working with independent random variables, variances add, standard deviations don't. But that's just my personal subjective preference which I mostly only use as a memory aid, feel free to ignore this paragraph. An much more indepth analysis can be read here . 
152	stats	 Label switching (i.e., the posterior distribution is invariant to switching component labels) is a problematic issue when using MCMC to estimate mixture models. Is there a standard (as in widely accepted) methodology to deal with the issue? If there is no standard approach then what are the pros and cons of the leading approaches to solve the label switching problem? 
153	stats	 The simple answer is that Likert scales are always ordinal. The intervals between positions on the scale are monotonic but never so well-defined as to be numerically uniform increments. That said, the distinction between ordinal and interval is based on the specific demands of the analysis being performed. Under special circumstances, you may be able to treat the responses as if they fell on an interval scale. To do this, typically the respondents need to be in close agreement regarding the meaning of the scale responses and the analysis (or the decisions made based on the analysis) should be relatively insensitive to problems that may arise. 
154	stats	 I am currently researching the trial roulette method for my masters thesis as an elicitation technique. This is a graphical method that allows an expert to represent her subjective probability distribution for an uncertain quantity. Experts are given counters (or what one can think of as casino chips) representing equal densities whose total would sum up to 1 - for example 20 chips of probability = 0.05 each. They are then instructed to arrange them on a pre-printed grid, with bins representing result intervals. Each column would represent their belief of the probability of getting the corresponding bin result. Example: A student is asked to predict the mark in a future exam. The figure below shows a completed grid for the elicitation of a subjective probability distribution. The horizontal axis of the grid shows the possible bins (or mark intervals) that the student was asked to consider. The numbers in top row record the number of chips per bin. The completed grid (using a total of 20 chips) shows that the student believes there is a 30% chance that the mark will be between 60 and 64.9. Some reasons in favour of using this technique are: Many questions about the shape of the expert's subjective probability distribution can be answered without the need to pose a long series of questions to the expert - the statistician can simply read off density above or below any given point, or that between any two points. During the elicitation process, the experts can move around the chips if unsatisfied with the way they placed them initially - thus they can be sure of the final result to be submitted. It forces the expert to be coherent in the set of probabilities that are provided. If all the chips are used, the probabilities must sum to one. Graphical methods seem to provide more accurate results, especially for participants with modest levels of statistical sophistication. 
155	stats	 I really enjoy hearing simple explanations to complex problems. What is your favorite analogy or anecdote that explains a difficult statistical concept? My favorite is Murray's explanation of cointegration using a drunkard and her dog. Murray explains how two random processes (a wandering drunk and her dog, Oliver) can have unit roots but still be related (cointegrated) since their joint first differences are stationary. The drunk sets out from the bar, about to wander aimlessly in random-walk fashion. But periodically she intones "Oliver, where are you?", and Oliver interrupts his aimless wandering to bark. He hears her; she hears him. He thinks, "Oh, I can't let her get too far off; she'll lock me out." She thinks, "Oh, I can't let him get too far off; he'll wake me up in the middle of the night with his barking." Each assesses how far away the other is and moves to partially close that gap. 
156	stats	 I know this must be standard material, but I had difficulty in finding a proof in this form. Let $e$ be a standard white Gaussian vector of size $N$. Let all the other matrices in the following be constant. Let $v = Xy + e$, where $X$ is an $N\times L$ matrix and $y$ is an $N\times 1$ vector, and let $$\left\{\begin{align}&#xD; \bar y &amp;= (X^TX)^{-1}X^Tv\\&#xD; \bar e &amp;= v - X\bar y&#xD; \end{align}\right.\quad.$$ If $c$ is any constant vector, $J = N - \mathrm{rank}(X)$, and $$\left\{\begin{align}&#xD; u &amp;= c^T\bar y\\&#xD; s^2 &amp;= \bar e^T\bar ec^T(X^TX)^{-1}c&#xD; \end{align}\right.\quad,$$ then the random variable defined as $t = u/\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom. I would be grateful if you could provide an outline for its proof. 
157	stats	 Definitely the Monty Hall Problem. 
159	stats	 Junk Charts is always interesting and thought-provoking, usually providing both criticism of visualizations in the popular media and suggestions for improvements. 
160	stats	 Dataspora , a data science blog. 
161	stats	 Econometricians often talk about a time series being integrated with order k, I(k) . k being the minimum number of differences required to obtain a stationary time series. What methods or statistical tests can be used to determine, given a level of confidence, the order of integration of a time series? 
162	stats	 If you carved your distribution (histogram) out of wood, and tried to balance it on your finger, the balance point would be the mean, no matter the shape of the distribution. If you put a stick in the middle of your scatter plot, and attached the stick to each data point with a spring, the resting point of the stick would be your regression line. [1] [1] this would technically be principal components regression. you would have to force the springs to move only "vertically" to be least squares, but the example is illustrative either way. 
163	stats	 Definition: A random variable is a measurable function from a probability space into a measurable space known as the state space. Example: Lets say I roll a fair six-sided die, with outcomes being one of the following: 1, 2, 3, 4, 5, or 6. Whichever number the die lands on is the number of free text-books I will give you. In this case, the final amount of free text books that I give you is the random variable because its value is based on the outcome (1, 2, 3, 4, 5, or 6 free text books) of a random event (rolling the die) and is not known before I roll the die. 
164	stats	 For governmental data: US: World: 
165	stats	 Maybe the concept, why it's used, and an example. 
166	stats	 Australia is currently having an election and understandably the media reports new political poll results daily. In a country of 22 million what percentage of the population would need to be sampled to get a statistically valid result? Is it possible that using too large a sample could affect the results, or does statistical validity monotonically increase with sample size? 
167	stats	 Principal components are orthogonal by definition, so any pair of PCs will have zero correlation. However, PCA can be used in regression if there are a large number of explanatory variables. These can be reduced to a small number of principal components and used as predictors in a regression. 
168	stats	 For univariate kernel density estimators (KDE), I use Silverman's rule for calculating $h$: \begin{equation} 0.9 \min(sd, IQR/1.34)\times n^{-0.2} \end{equation} What are the standard rules for multivariate KDE (assuming a Normal kernel). 
169	stats	 For time series data, try the Time Series Data Library . 
170	stats	 Are there any free statistical textbooks available? 
171	stats	 There are a number of statistical tests (known as "unit root tests") for dealing with this problem. The most popular is probably the "Augmented Dickey-Fuller" (ADF) test, although the Phillips-Perron (PP) test and the KPSS test are also widely used. Both the ADF and PP tests are based on a null hypothesis of a unit root (i.e., an I(1) series). The KPSS test is based on a null hypothesis of stationarity (i.e., an I(0) series). Consequently, the KPSS test can give quite different results from the ADF or PP tests. 
172	stats	 Sample size doesn't much depend on the population size, which is counter-intuitive to many. Most polling companies use 400 or 1000 people in their samples. There is a reason for this: A sample size of 400 will give you a confidence interval of +/-5% 19 times out of 20 (95%) A sample size of 1000 will give you a confidence interval of +/-3% 19 times out of 20 (95%) When you are measuring a proportion near 50% anyways. This calculator isn't bad: 
173	stats	 I recently started working for a tuberculosis clinic. We meet periodically to discuss the number of TB cases we're currently treating, the number of tests administered, etc. I'd like to start modeling these counts so that we're not just guessing whether something is unusual or not. Unfortunately, I've had very little training in time series, and most of my exposure has been to models for very continuous data (stock prices) or very large numbers of counts (influenza). But we deal with 0-18 cases per month (mean 6.68, median 7, var 12.3), which are distributed like this: [image lost to the mists of time] [image eaten by a grue] I've found a few articles that address models like this, but I'd greatly appreciate hearing suggestions from you - both for approaches and for R packages that I could use to implement those approaches. EDIT: mbq's answer has forced me to think more carefully about what I'm asking here; I got too hung-up on the monthly counts and lost the actual focus of the question. What I'd like to know is: does the (fairly visible) decline from, say, 2008 onward reflect a downward trend in the overall number of cases? It looks to me like the number of cases monthly from 2001-2007 reflects a stable process; maybe some seasonality, but overall stable. From 2008 through the present, it looks like that process is changing: the overall number of cases is declining, even though the monthly counts might wobble up and down due to randomness and seasonality. How can I test if there's a real change in the process? And if I can identify a decline, how could I use that trend and whatever seasonality there might be to estimate the number of cases we might see in the upcoming months? Whew. Thanks for bearing with me. 
174	stats	 The most widely used and probably the best of what is available is Other online stats books include Update: I can now add my own forecasting textbook Forecasting: principles and practice (Hyndman &amp; Athanasopoulos, 2012) 
175	stats	 Often times a statistical analyst is handed a set dataset and asked to fit a model using a technique such as linear regression. Very frequently the dataset is accompanied with a disclaimer similar to "Oh yeah, we messed up collecting some of these data points -- do what you can". This situation leads to regression fits that are heavily impacted by the presence of outliers that may be erroneous data. Given the following: It is dangerous from both a scientific and moral standpoint to throw out data for no reason other than it "makes the fit look bad". In real life, the people who collected the data are frequently not available to answer questions such as "when generating this data set, which of the points did you mess up, exactly?" What statistical tests or rules of thumb can be used as a basis for excluding outliers in linear regression analysis? Are there any special considerations for multilinear regression? 
176	stats	 Let us say a man rolls a six sided die and it has outcomes 1, 2, 3, 4, 5, or 6. Furthermore, he says that if it lands on a 3, he'll give you a free text book. Then informally: The Frequentist would say that each outcome has an equal 1 in 6 chance of occurring. She views probability as being derived from long run frequency distributions. The Bayesian however would say hang on a second, I know that man, he's David Blaine, a famous trickster! I have a feeling he's up to something. I'm going to say that there's only a 1% chance of it landing on a 3 BUT I'll re-evaluate that beliefe and change it the more times he rolls the die. If I see the other numbers come up equally often, then I'll iteratively increase the chance from 1% to something slightly higher, otherwise I'll reduce it even further. She views probability as degrees of belief in a proposition. 
177	stats	 Rather than exclude outliers, you can use a robust method of regression. In R, for example, the function from the MASS package can be used instead of the function. The method of estimation can be tuned to be more or less robust to outliers. 
178	stats	 RapidMiner for data and text mining 
179	stats	 For a univariate KDE, you are better off using something other than Silverman's rule which is based on a normal approximation. One excellent approach is the Sheather-Jones method, easily implemented in R; for example, The situation for multivariate KDE is not so well studied, and the tools are not so mature. Rather than a bandwidth, you need a bandwidth matrix. To simplify the problem, most people assume a diagonal matrix, although this may not lead to the best results. The ks package in R provides some very useful tools including allowing a full (not necessarily diagonal) bandwidth matrix. 
180	stats	 I really like the FRED , from the St. Louis Fed (economics data). You can chart the series or more than one series, you can do some transformations to your data and chart it, and the NBER recessions are shaded. 
181	stats	 Is there a standard and accepted method for selecting the number of layers, and the number of nodes in each layer, in a FF NN? I'm interested in automated ways of building neural networks. 
182	stats	 Sometimes outliers are bad data, and should be excluded, such as typos. Sometimes they are Wayne Gretzky or Michael Jordan, and should be kept. Outlier detection methods include: Univariate -&gt; boxplot. outside of 1.5 times inter-quartile range is an outlier. Bivariate -&gt; scatterplot with confidence ellipse. outside of, say, 95% confidence ellipse is an outlier. Multivariate -&gt; Mahalanobis D2 distance Mark those observations as outliers. Run a logistic regression (on Y=IsOutlier) to see if there are any systematic patterns. Remove ones that you can demonstrate they are not representative of any sub-population. 
183	stats	 I need to analyze the 100k MovieLens dataset for clustering with two algorithms of my choice, between the likes of k-means, agnes, diana, dbscan, and several others. What tools (like Rattle, or Weka) would be best suited to help me make some simple clustering analysis over this dataset? 
184	stats	 Try using the function for time series decomposition. It provides a very flexible method for extracting a seasonal component from a time series. 
185	stats	 A great introductory text covering the topics you mentioned is Introduction to Information Retrieval , which is available online in full text for free. 
187	stats	 As far as I know there is no way to select automatically the number of layers and neurons in each layer. But there are networks that can build automatically their topology, like EANN (Evolutionary Artificial Neural Networks, which use Genetic Algorithms to evolved the topology). There are several approaches, a more or less modern one that seemed to give good results was NEAT (Neuro Evolution of Augmented Topologies). You can get more info: 
188	stats	 I'd probably say something like this: "Anytime we want to talk about probabilities, we're really integrating a density. In Bayesian analysis, a lot of the densities we come up with aren't analytically tractable: you can only integrate them -- if you can integrate them at all -- with a great deal of suffering. So what we do instead is simulate the random variable a lot, and then figure out probabilities from our simulated random numbers. If we want to know the probability that X is less than 10, we count the proportion of simulated random variable results less than 10 and use that as our estimate. That's the "Monte Carlo" part, it's an estimate of probability based off of random numbers. With enough simulated random numbers, the estimate is very good, but it's still inherently random. "So why "Markov Chain"? Because under certain technical conditions, you can generate a memoryless process (aka a Markovian one) that has the same limiting distribution as the random variable that you're trying to simulate. You can iterate any of a number of different kinds of simulation processes that generate correlated random numbers (based only on the current value of those numbers), and you're guaranteed that once you pool enough of the results, you will end up with a pile of numbers that looks "as if" you had somehow managed to take independent samples from the complicated distribution you wanted to know about. "So for example, if I want to estimate the probability that a standard normal random variable was less than 0.5, I could generate ten thousand independent realizations from a standard normal distribution and count up the number less than 0.5; say I got 6905 that were less than 0.5 out of 10000 total samples; my estimate for P(Z&lt;0.5) would be 0.6905, which isn't that far off from the actual value. That'd be a Monte Carlo estimate. "Now imagine I couldn't draw independent normal random variables, instead I'd start at 0, and then with every step add some uniform random number between -0.5 and 0.5 to my current value, and then decide based on a particular test whether I liked that new value or not; if I liked it, I'd use the new value as my current one, and if not, I'd reject it and stick with my old value. Because I only look at the new and current values, this is a Markov chain. If I set up the test to decide whether or not I keep the new value correctly (it'd be a random walk Metropolis-Hastings, and the details get a bit complex), then even though I never generate a single normal random variable, if I do this procedure for long enough, the list of numbers I get from the procedure will be distributed like a large number of draws from something that generates normal random variables. This would give me a Markov Chain Monte Carlo simulation for a standard normal random variable. If I used this to estimate probabilities, that would be a MCMC estimate." 
189	stats	 Let $F(x)$ denote the cdf; then you can always approximate the pdf of a continuous random variable by calculating $$ \frac{F(x_2) - F(x_1)}{x_2 - x_1},$$ where $x_1$ and $x_2$ are on either side of the point where you want to know the pdf and the distance $|x_2 - x_1|$ is small. 
190	stats	 A New View of Statistics by Will G. Hopkins is great! It is designed to help you understand how to understand the results of statistical analyses, not how to prove statistical theorems. 
191	stats	 The Bland-Altman plot is more widely known as the Tukey Mean-Difference Plot (one of many charts devised by John Tukey ). The idea is that x-axis is the mean of your two measurements, which is your best guess as to the "correct" result and the y-axis is the difference between the two measurement differences. The chart can then highlight certain types of anomalies in the measurements. For example, if one method always gives too high a result, then you'll get all of your points above or all below the zero line. It can also reveal, for example, that one method over-estimates high values and under-estimates low values. If you see the points on the Bland-Altman plot scattered all over the place, above and below zero, then the suggests that there is no consistent bias of one approach versus the other (of course, there could be hidden biases that this plot does not show up). Essentially, it is a good first step for exploring the data. Other techniques can be used to dig into more particular sorts of behaviour of the measurements. 
192	stats	 I'm aware that this one is far from yes or no question, but I'd like to know which techniques do you prefer in categorical data analysis - i.e. cross tabulation with two categorical variables. I've come up with: &chi; 2 test - well, this is quite self-explanatory Fisher's exact test - when n &lt; 40, Yates' continuity correction - when n &gt; 40, Cramer's V - measure of association for tables which have more than 2 x 2 cells, &Phi; coefficient - measure of association for 2 x 2 tables, contingency coefficient (C) - measure of association for n x n tables, odds ratio - independence of two categorical variables, McNemar marginal homogeniety test, And my question here is: Which statistical techniques for cross-tabulated data (two categorical variables) do you consider relevant (and why)? 
193	stats	 Suppose that you want to know what percentage of people would vote for a particular candidate (say, $\pi$, note that by definition $\pi$ is between 0 and 100). You sample $N$ voters at random to find out how they would vote and your survey of these $N$ voters tells you that the percentage is $p$. So, you would like to establish a confidence interval for the true percentage. If you assume that $p$ is normally distributed (an assumption that may or may not be justified depending on how 'big' $N$ is) then your confidence interval for $\pi$ would be of the following form: $$ CI = [ p - k * sd(p),~~ p + k * sd(p)] $$ where $k$ is a constant that depends on the extent of confidence you want (i.e., 95% or 99% etc). From a polling perspective, you want the width of your confidence interval to be 'low'. Usually, pollsters work with the margin of error which is basically one-half of the CI. In other words, $\text{MoE} = k * sd(p)$. Here is how we would go about calculating $sd(p)$: By definition, $p = \sum X_i / N$ where, $X_i = 1$ if voter $i$ votes for candidate and $0$ otherwise. Since, we sampled the voters at random, we could assume that $X_i$ is a i.i.d Bernoulli random variable. Therefore, $$ Var(P) = V\left( \sum\frac{X_i}{N}\right) = \frac{\sum V(X_i)}{N^2} = \frac{N \pi (1-\pi)}{N^2} = \frac{\pi (1-\pi)}{N}. $$ Thus, $$ sd(p) = \sqrt{\frac{\pi * (1-\pi)}{N}} $$ Now to estimate margin of error we need to know $\pi$ which we do not know obviously. But, an inspection of the numerator suggests that the 'worst' estimate for $sd(p)$ in the sense that we get the 'largest' standard deviation is when $\pi = 0.5$. Therefore, the worst possible standard deviation is: $$ sd(p) = \sqrt{0.5 * 0.5 / N } = 0.5 / \sqrt{N} $$ So, you see that the margin of error falls off exponentially with $N$ and thus you really do not need very big samples to reduce your margin of error, or in other words $N$ need not be very large for you to obtain a narrow confidence interval. For example, for a 95 % confidence interval (i.e., $k= 1.96$) and $N = 1000$, the confidence interval is: $$ \left[p - 1.96 \frac{0.5}{\sqrt{1000}},~~ p + 1.96 \frac{0.5}{\sqrt{1000}}\right] = [p - 0.03,~~ p + 0.03] $$ As we increase $N$ the costs of polling go up linearly but the gains go down exponentially. That is the reason why pollsters usually cap $N$ at 1000 as that gives them a reasonable error of margin under the worst possible assumption of $\pi = 50\%$. 
194	stats	 I am sure that everyone who's trying to find patterns in historical stock market data or betting history would like to know about this. Given a huge sets of data, and thousands of random variables that may or may not affect it, it makes sense to ask any patterns that you extract out from the data are indeed true patterns, not statistical fluke. A lot of patterns are only valid when they are tested in the samples. And even those that are patterns that are valid out of samples may cease to become valid when you apply it in the real world. I understand that it is not possible to completely 100% make sure a pattern is valid all the time, but besides in and out of samples tests, are their any tests that could establish the validness of a pattern? 
195	stats	 I am looking at fitting distributions to data (with a particular focus on the tail) and am leaning towards Anderson-Darling tests rather than Kolmogorov-Smirnov. What do you think are the relative merits of these or other tests for fit (e.g. Cramer-von Mises)? 
196	stats	 Besides gnuplot and ggobi , what open source tools are people using for visualizing multi-dimensional data? Gnuplot is more or less a basic plotting package. Ggobi can do a number of nifty things, such as: animate data along a dimension or among discrete collections animate linear combinations varying the coefficients compute principal components and other transformations visualize and rotate 3 dimensional data clusters use colors to represent a different dimension What other useful approaches are based in open source and thus freely reusable or customizable? Please provide a brief description of the package's abilities in the answer. 
197	stats	 How about R with ggplot2 ? Other tools that I really like: Processing Prefuse Protovis 
198	stats	 Start with the distribution of $\bar{y}$, show that since $v$ is normal, $\bar{y}$ is multivariate normal and that consequently $u$ must also be a multivariate normal; also show that the covariance matrix of $\bar{y}$ is of the form $\sigma^2\cdot(X^T X)^{-1}$ and thus -- if $\sigma^2$ were known -- the variance of $u$ would be $\sigma^2 c^T (X^T X)^{-1} c$. Show that the distribution of $\bar{e}^T \bar{e}$ must be chi-squared and ( carefully ) find the degrees of freedom. Think about how what the operation $\bar{e}^T \bar{e} c^T (X^T X)^{-1} c$ must therefore produce, and what it's distribution and degrees of freedom are. The result follows (almost) immediately from the definition of the t-distribution. 
199	stats	 You could try: Bagging Boosting Cross validation ) 
200	stats	 If you want to know that a pattern is meaningful, you need to show what it actually means . Statistical tests do not do this. Unless your data can be said to be in some sense "complete", inferences draw from the data will always be provisional. You can increase your confidence in the validity of a pattern by testing against more and more out of sample data, but that doesn't protect you from it turning out to be an artefact. The broader your range of out of sample data -- eg, in terms of how it is acquired and what sort of systematic confounding factors might exist within it -- the better the validation. Ideally, though, you need to go beyond identifying patterns and come up with a persuasive theoretical framework that explains the patterns you've found, and then test that by other, independent means. (This is called "science".) 
201	stats	 Start R and type . This will show all datasets in the search path. Many additional datasets are available in add-on packages. For example, there are some interesting real-world social science datasets in the package. 
202	stats	 If you like learning through videos, I collated a list of R training videos . I also prepared a general post on learning R with suggestions on books, online manuals, blogs, videos, user interfaces, and more. 
203	stats	 Following on from this question : Imagine that you want to test for differences in central tendency between two groups (e.g., males and females) on a 5-point Likert item (e.g., satisfaction with life: Dissatisfied to Satisfied). I think a t-test would be sufficiently accurate for most purposes, but that a bootstrap test of differences between group means would often provide more accurate estimate of confidence intervals. What statistical test would you use? 
204	stats	 The lattice package in R. Lattice is a powerful and elegant high-level data visualization system, with an emphasis on multivariate data,that is sufﬁcient for typical graphics needs, and is also ﬂexible enough to handle most nonstandard requirements. Quick-R has a quick introduction . 
205	stats	 I'm curious about why we treat fitting GLMS as though they were some special optimization problem. Are they? It seems to me that they're just maximum likelihood, and that we write down the likelihood and then ... we maximize it! So why do we use Fisher scoring instead of any of the myriad of optimization schemes that has been developed in the applied math literature? 
206	stats	 What is the difference between discrete data and continuous data? 
207	stats	 First, we need to understand what is a markov chain. Consider the following weather example from Wikipedia. Suppose that weather on any given day can be classified into two states only: sunny and rainy. Based on past experience, we know the following: Probability(Next day is sunny | Given today is rainy ) = 0.50 Since, the next day's weather is either sunny or rainy it follows that: Probability(Next day is Rainy | Given today is rainy ) = 0.50 Similarly, let: Probability(Next day is rainy | Given today is sunny ) = 0.10 Therefore, it follows that: Probability(Next day is sunny | Given today is sunny ) = 0.90 The above four numbers can be compactly represented as a transition matrix which represents the probabilities of the weather moving from one state to another state as follows: We might ask several questions whose answers follow: Q1: If the weather is sunny today then what is the weather likely to be tomorrow? A1: Since, we do not know what is going to happen for sure, the best we can say is that there is a 90% chance that it is likely to be sunny and 10% that it will be rainy. Q2: What about two days from today? A2: One day prediction: 90% sunny, 10% rainy. Therefore, two days from now: First day it can be sunny and the next day also it can be sunny. Chances of this happening are: 0.9 x 0.9. Or First day it can be rainy and second day it can be sunny. Chances of this happening are: 0.1 x 0.5 Therefore, the probability that the weather will be sunny in two days is: Prob(Sunny two days from now) = 0.9 x 0.9 + 0.1 x 0.5 = 0.81 + 0.05 = 0.86 Similarly, the probability that it will be rainy is: Prob(Rainy two days from now) = 0.1 x 0.5 + 0.9 x 0.1 = 0.05 + 0.09 = 0.14 In linear algebra (transition matrices) these calculations correspond to all the permutations in transitions from one step to the next (sunny-to-sunny ($S_2S$), sunny-to-rainy ($S_2R$), rainy-to-sunny ($R_2S$) or rainy-to-rainy ($R_2R$)) with their calculated probabilities: On the lower part of the image we see how to calculate the probability of a future state ($t+1$ or $t+2$) given the probabilities (probability mass function, $PMF$) for every state (sunny or rainy) at time zero (now or $t_0$) as simple matrix multiplication. If you keep forecasting weather like this you will notice that eventually the nth day forecast where n is very large (say 30) settles to the following 'equilibrium' probabilities: Prob(Sunny) = 0.833 Prob(Rainy) = 0.167 In other words, your forecast for the nth day and the n+1th day remain the same. In addition, you can also check that the 'equilibrium' probabilities do not depend on the weather today. You would get the same forecast for the weather if you start of by assuming that the weather today is sunny or rainy. The above example will only work if the state transition probabilities satisfy several conditions which I will not discuss here. But, notice the following features of this 'nice' markov chain (nice = transition probabilities satisfy conditions): Irrespective of the initial starting state we will eventually reach an equilibrium probability distribution of states. Markov Chain Monte Carlo exploits the above feature as follows: We want to generate random draws from a target distribution. We then identify a way to construct a 'nice' markov chain such that its equilibrium probability distribution is our target distribution. If we can construct such a chain then we arbitrarily start from some point and iterate the markov chain many times (like how we forecasted the weather n times). Eventually, the draws we generate would appear as if they are coming from our target distribution. We then approximate the quantities of interest (e.g. mean) by taking the sample average of the draws after discarding a few initial draws which is the monte carlo component. There are several ways to construct 'nice' markov chains (e.g., gibbs sampler, Metropolis-Hastings algorithm). 
208	stats	 Temperatures are continuous. It can be 23 degrees, 23.1 degrees, 23.100004 degrees. Gender is discrete. You can only be male or female (insert san francisco joke here). Something you would represent with a whole number like 0, 1, 2, etc The difference is important as many statistical and data mining algorithms can handle one type but not the other. For example in regular regression, the Y must be continuous. In logistic regression the Y is discrete. 
209	stats	 Discrete data can only take particular values. There may potentially be an infinite number of those values, but each is distinct and there's no grey area in between. Discrete data can be numeric -- like numbers of apples -- but it can also be categorical -- like red or blue, or male or female, or good or bad. Continuous data are not restricted to defined separate values, but can occupy any value over a continuous range. Between any two continuous data values there may be an infinite number of others. Continuous data are always essentially numeric. It sometimes makes sense to treat numeric data that is properly of one type as being of the other. For example, something like height is continuous, but often we don't really care too much about tiny differences and instead group heights into a number of discrete bins . Conversely, if we're counting large amounts of some discrete entity -- grains of rice, or termites, or pennies in the economy -- we may choose not to think of 2,000,006 and 2,000,008 as crucially different values but instead as nearby points on an approximate continuum. It can also sometimes be useful to treat numeric data as categorical, eg: underweight, normal, obese. This is usually just another kind of binning. It seldom makes sense to consider categorical data as continuous. 
210	stats	 Data is always discrete. Given a sample of values on a variable, the maximum number of distinct values the variable can take is equal to . See this quote All actual sample spaces are discrete, and all observable random variables have discrete distributions. The continuous distribution is a mathematical construction, suitable for mathematical treatment, but not practically observable. E.J.G. Pitman (1979, p. 1). Data on a variable are typically assumed to be drawn from a random variable. The random variable is continuous over a range if there is an infinite number of possible values that the variable can take between any two different points in the range. For example, height, weight, and time are typically assumed to be continuous. Of course, any measurement of these variables will be finitely accurate and in some sense discrete. It is useful to distinguish between ordered (i.e., ordinal), unordered (i.e., nominal), and binary discrete variables. Some introductory textbooks confuse a continuous variable with a numeric variable. For example, a score on a computer game is discrete even though it is numeric. Some introductory textbooks confuse a ratio variable with continuous variables. A count variable is a ratio variable, but it is not continuous. In actual practice, a variable is often treated as continuous when it can take on a sufficiently large number of different values. References Pitman, E. J. G. 1979. Some basic theory for statistical inference. London: Chapman and Hall. Note: I found the quote in the introduction of Chapter 2 of Murray Aitkin's book Statistical Inference: An Integrated Bayesian/Likelihood Approach 
211	stats	 I have written a document that is freely available at my website and on CRAN. See the linked page: icebreakeR The datasets that are used in the document are also linked from that page. Feedback is welcome and appreciated! Andrew 
212	stats	 I have 2 ASR (Automatic Speech Recognition) models, providing me with text transcriptions for my testdata. The error measure I use is Word Error Rate. What methods do I have to test for statistical significance of my new results? An example: I have an experiment with 10 speaker, all having 100 (the same) sentences, total 900 words per speaker. Method A has an WER (word error rate) of 19.0%, Method B 18.5%. How do I test whether Method B is significantly better? 
213	stats	 Suppose I have a large set of multivariate data with at least three variables. How can I find the outliers? Pairwise scatterplots won't work as it is possible for an outlier to exist in 3 dimensions that is not an outlier in any of the 2 dimensional subspaces. I am not thinking of a regression problem, but of true multivariate data. So answers involving robust regression or computing leverage are not helpful. One possibility would be to compute the principal component scores and look for an outlier in the bivariate scatterplot of the first two scores. Would that be guaranteed to work? Are there better approaches? 
