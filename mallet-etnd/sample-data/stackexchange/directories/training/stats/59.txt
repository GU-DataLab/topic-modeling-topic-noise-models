 The assumption of normality is just the supposition that the underlying random variable of interest is distributed normally , or approximately so. Intuitively, normality may be understood as the result of the sum of a large number of independent random events. More specifically, normal distributions are defined by the following function: where $\mu$ and $\sigma^2$ are the mean and the variance, respectively, and which appears as follows: This can be checked in multiple ways , that may be more or less suited to your problem by its features, such as the size of n. Basically, they all test for features expected if the distribution were normal (e.g. expected quantile distribution ). 
